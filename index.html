<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Project1: Images of the Russian Empire ‚Äî Part 0</title>
  <style>
    body {
      background-color: #D8C3DD; /* Á¥´Ëâ≤ËÉåÊôØÔºà‰øùÊåÅÂéüÊù•È¢úËâ≤Ôºâ */
      font-family: Arial, sans-serif;
      color: white;
      margin: 0;
      padding: 0;
    }
    /* ÂØºËà™Ê†èÔºàÈ¢úËâ≤‰∏éÊ†∑Âºè‰øùÊåÅ‰∏çÂèòÔºâ */
    nav {
      background: rgba(0, 0, 0, 0.3);
      padding: 10px 20px;
      position: sticky;
      top: 0;
      z-index: 1000;
    }
    nav a {
      color: #ffddff;
      text-decoration: none;
      margin: 0 15px;
      font-weight: bold;
    }
    nav a:hover {
      text-decoration: underline;
    }
    .container {
      width: 80%;
      margin: auto;
      padding: 20px;
    }
    h1 {
      text-align: center;
      margin-top: 20px;
    }
    .section {
      background: rgba(0,0,0,0.2);
      padding: 20px;
      margin: 40px 0;
      border-radius: 12px;
    }
    .section h2 {
      color: #ffddff;
      text-align: center;
      margin-top: 0;
    }
    p {
      text-align: left;
      max-width: 1000px;
      margin: auto;
      margin-bottom: 20px;
      font-size: 18px;
      color: white;
    }
    hr {
      border: none;
      border-top: 1px solid rgba(255,255,255,0.12);
      margin: 30px 0;
    }

    /* ÂõæÁâáÊéíÁâàÂæÆË∞ÉÔºàÊõ¥Â§ß„ÄÅÊõ¥Â±Ö‰∏≠„ÄÅÈó¥Ë∑ùÊõ¥Â§ßÔºâ */
    .two-images {
      display: flex;
      justify-content: center;
      gap: 48px; /* ‰∏≠Èó¥Á©∫‰∏ÄÁÇπ */
      margin-top: 30px;
      flex-wrap: wrap;
    }
    .img-block {
      flex: 0 0 320px; /* Âü∫ÂáÜÊõ¥Â§ß */
      text-align: center;
    }
    .img-block img {
      width: 400px; /* ÊîæÂ§ßÂõæÁâá */
      border-radius: 8px;
      border: 1px solid rgba(255,255,255,0.08);
    }
    .img-block p {
      margin-top: 8px;
      font-size: 16px;
      color: white;
      text-align: center;
    }

    /* ‰øùËØÅÂ∞èÂ±èÂπïÈÄÇÈÖç */
    @media (max-width: 760px) {
      .container { width: 92%; padding: 12px; }
      .img-block { flex: 0 0 45%; }
      .img-block img { width: 100%; }
    }
  </style>
  <!-- MathJax kept if needed later -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <nav>
    <a href="#part0">Part0</a>
    <a href="#part1">Part1</a>
    <a href="#part2">Part2</a>
    <a href="#part3">Part3</a>
  </nav>

  <div class="container">
    <h1>Project1: Images of the Russian Empire ‚Äî Calibration & 3D Scan (Part 0)</h1>

    <!-- =========================== -->
    <!-- Part 0.0 -->
    <!-- =========================== -->
    <div class="section" id="part0">
      <h2>Part 0.0: Calibrating My Camera and Capturing a 3D Scan</h2>

      <h2 id="part0-1">Part 0.1: Calibrating My Camera</h2>

      <p>
        I first captured 33 calibration images
        containing multiple ArUco markers, keeping the zoom fixed while varying the
        viewing angle and distance.
      </p>

      <p>
        For each ArUco tag, I defined a local 3D coordinate system on the tag plane to represent its real-world coordinates. 
        I then iterated over all calibration images and used OpenCV‚Äôs ArUco detector to find the 2D pixel coordinates of the detected tag corners. 
        Images where no markers were detected were skipped.
      </p>

      <p>
        After collecting all 2D‚Äì3D correspondences across the dataset, I used cv2.calibrateCamera() to estimate the camera intrinsic matrix and distortion coefficients. 
        These calibration parameters are subsequently used to compute object poses for 3D scanning.
      </p>

      <hr>

      <h2 id="part0-2">Part 0.2: Capturing the 3D Scan</h2>

      <p>
        With camera intrinsics estimated, I proceeded to capture image data for the 3D scan.
        I photographed the object from many viewpoints with substantial overlap (roughly 60‚Äì80%),
        maintaining consistent exposure and diffuse lighting to avoid harsh shadows. When possible,
        I placed the object on a simple turntable and rotated it incrementally; otherwise I walked
        the camera around the object while keeping the object centered in the frame.
      </p>

      <p>
        For robust reconstruction I captured images at multiple elevation angles (top, mid, low)
        and ensured good coverage of concave regions. I checked for motion blur and removed any
        blurry frames. All photos were saved in a single folder and named sequentially to preserve
        ordering.
      </p>

      <p>
        In post-processing I undistorted each image using the previously computed distortion
        coefficients, so the reconstruction algorithm receives geometrically-correct images.
        I then fed the undistorted image set and the camera intrinsics into a photogrammetry
        pipeline (e.g., COLMAP / OpenMVG + OpenMVS) to compute camera poses, sparse and dense
        point clouds, and finally a textured mesh (PLY/OBJ). The resulting 3D model can be
        aligned with the calibration coordinate frame so that measured 3D points correspond to
        the real-world scale used during calibration.
      </p>

      <p>
        This calibrated scan workflow ensures that the 3D geometry and the camera parameters are
        mutually consistent, which is important for later steps such as precise pose estimation,
        metric measurements, or combining multi-modal data.
      </p>
    </div>

    <!-- =========================== -->
    <!-- Part 0.3 -->
    <!-- =========================== -->
    <div class="section" id="part0-3-section">
      <h2 id="part0-3">Part 0.3: Estimating Camera Pose</h2>

      <p>
        With the camera intrinsics calibrated in Part 0.1, I next estimated the camera pose
        (its 3D position and orientation) for every image in my object scan. This is a classic
        Perspective-n-Point (PnP) problem: given several known 3D points and their 2D projections,
        we solve for the camera's extrinsic parameters.
      </p>

      <p>
        In my case, each scan image contains a single ArUco tag placed next to the object.
        For every image, I used OpenCV‚Äôs ArUco detector to obtain the 2D pixel coordinates of the
        tag‚Äôs four corners. Since the physical size of my printed tag is known, I defined its
        corresponding 3D corner coordinates in meters on a flat z=0 plane.
      </p>

      <p>
        Using these 2D‚Äì3D correspondences, I called <code>cv2.solvePnP()</code> to compute the tag-to-camera
        rotation and translation. I then converted the returned axis-angle vector to a rotation matrix 
        via <code>cv2.Rodrigues()</code>. Since OpenCV gives the world-to-camera transform, I inverted it to
        obtain the camera-to-world (c2w) pose matrix required for visualization and NeRF-style pipelines.
      </p>

      <!-- Example images for Part 0.3 (two images, centered, larger) -->
      <div class="two-images" role="group" aria-label="Pose examples">
        <div class="img-block">
          <img src="pose1.jpg" alt="Pose Visualization 1">
          <p>Estimated Pose (Example 1)</p>
        </div>

        <div class="img-block">
          <img src="pose2.jpg" alt="Pose Visualization 2">
          <p>Estimated Pose (Example 2)</p>
        </div>
      </div>
    </div>

    <hr>

    <!-- =========================== -->
    <!-- Part 0.4 -->
    <!-- =========================== -->
    <div class="section" id="part0-4-section">
      <h2 id="part0-4">Part 0.4: Undistorting Images and Building the Dataset</h2>

      <p>
        After estimating the pose for each image, I prepared the final dataset by undistorting all
        images using the calibration parameters. Since NeRF assumes a perfect pinhole camera model,
        removing radial and tangential distortion is important for geometric consistency.
      </p>

      <p>
        I used <code>cv2.undistort()</code> together with the calibrated intrinsics and distortion 
        coefficients to obtain clean, geometrically-correct images. Some images exhibit black borders 
        after undistortion; for these cases, I optionally used 
        <code>cv2.getOptimalNewCameraMatrix()</code> to compute a cropped intrinsics matrix and a valid ROI,
        ensuring that the principal point is updated whenever cropping occurs.
      </p>

      <p>
        Finally, I organized all undistorted images, camera intrinsics, and camera-to-world pose matrices
        into a structured dataset (following the JSON / transforms schema used in NeRF pipelines).  
        This dataset will serve as the input for dense reconstruction and rendering in later parts
        of the project.
      </p>
    </div>

<!-- =========================== -->
<!-- Part 1 -->
<!-- =========================== -->
<div class="section" id="part1">
  <h2>Part 1: Fit a Neural Field to a 2D Image</h2>

  <p>
    Before moving to a full 3D Neural Radiance Field (NeRF), this section uses a
    <b>2D Neural Field</b> to build intuition. In the 2D case there is no notion of radiance or
    volume density, so NeRF simplifies into a function that maps a 2D pixel coordinate
    <i>(x, y)</i> to an RGB color <i>(r, g, b)</i>. The goal is to learn this function using a
    small MLP with positional encoding.
  </p>

  <p>
    I followed the assignment instructions and implemented the following components:
  </p>

  <p>
    <b>1. Positional Encoding (PE)</b><br>
    I applied sinusoidal positional encoding to each 2D coordinate, following the formulation
    from the NeRF paper. With a maximum frequency level of <i>L = 10</i>, the original
    2D input was expanded to a 42-dimensional vector. High-frequency PE allows the network
    to reproduce sharp edges and fine structures in the target image.
  </p>

  <p>
    <b>2. Multilayer Perceptron (MLP)</b><br>
    I implemented an MLP that takes the encoded coordinates as input and outputs a
    3-dimensional RGB value. The network ends with a <code>Sigmoid</code> layer to ensure that the
    predicted colors fall within <i>[0, 1]</i>. The input image was also normalized to this range
    for supervision.
  </p>

  <p>
    <b>3. Dataloader with Random Pixel Sampling</b><br>
    To avoid GPU memory issues when training on high-resolution images, I wrote a dataloader
    that randomly samples <i>10,000</i> pixels every iteration. The dataloader returns normalized
    pixel coordinates <i>(x / W, y / H)</i> and their corresponding RGB values.
  </p>

  <p>
    <b>4. Loss Function, Optimizer, and Metrics</b><br>
    The network was trained using <code>MSELoss</code> between predicted and ground-truth colors.
    I used the Adam optimizer with a learning rate of <code>1e-2</code> and trained for
    1000‚Äì3000 iterations. Reconstruction quality was evaluated using the Peak
    Signal-to-Noise Ratio (PSNR), computed from the MSE.
  </p>

  <p>
    <b>5. Hyperparameter Tuning</b><br>
    I experimented with different MLP widths and different PE frequency levels to see
    how they affect convergence and reconstruction sharpness. Below are the results after
    200, 800, 1400, and 2000 training iterations.
  </p>

  <hr>

  <!-- Image row (4 images) -->
  <div style="
      display: flex;
      justify-content: center;
      gap: 25px;
      margin-top: 20px;
  ">
    <div style="text-align: center;">
      <img src="fox200.png" width="200">
      <p style="margin-top: 8px;">Iteration 200</p>
    </div>

    <div style="text-align: center;">
      <img src="fox800.png" width="200">
      <p style="margin-top: 8px;">Iteration 800</p>
    </div>

    <div style="text-align: center;">
      <img src="fox1400.png" width="200">
      <p style="margin-top: 8px;">Iteration 1400</p>
    </div>

    <div style="text-align: center;">
      <img src="fox2000.png" width="200">
      <p style="margin-top: 8px;">Iteration 2000</p>
    </div>
  </div>

<!-- =========================== -->
<!-- My own image -->
<!-- =========================== -->
<h3 style="text-align: center; margin-top: 40px;">My own image</h3>

<div style="
    display: flex;
    justify-content: center;
    gap: 30px;
    margin-top: 20px;
">
  <div style="text-align: center;">
    <img src="yifei200.png" width="200">
    <p style="margin-top: 8px;">Iteration 200</p>
  </div>

  <div style="text-align: center;">
    <img src="yifei800.png" width="200">
    <p style="margin-top: 8px;">Iteration 800</p>
  </div>

  <div style="text-align: center;">
    <img src="yifei1400.png" width="200">
    <p style="margin-top: 8px;">Iteration 1400</p>
  </div>

  <div style="text-align: center;">
    <img src="yifei2000.png" width="200">
    <p style="margin-top: 8px;">Iteration 2000</p>
  </div>
</div>
<!-- =========================== -->
<!-- Final Results / Hyperparameter Tuning -->
<!-- =========================== -->
<h3 style="text-align: center; margin-top: 40px;">Final Results for Different Hyperparameters</h3>

<p>
  I experimented with two choices of maximum positional encoding frequency (L) and two
  choices of MLP width (number of hidden channels). With a high frequency L = 10 and
  a wide network (width = 256), the network captures fine details and sharp edges very well.
  When the width is reduced to 32 while keeping L = 10, the main structures are still preserved
  but the output becomes slightly noisier and blurrier. Lowering the frequency to L = 2 makes
  the network only capable of representing smooth, low-frequency patterns, and reducing
  width to 32 further exaggerates this smoothing, making images almost piecewise planar.
</p>

<p>
  These trends are visible in both my own image and the example fox image. Below are the
  reconstructed results from four different hyperparameter settings, arranged in a 2√ó2 grid
  (2 images per row).
</p>

<!-- 2x2 image grid, 2 per row -->
<div style="display: flex; justify-content: center; gap: 40px; flex-wrap: wrap; margin-top: 20px;">

  <!-- First row -->
  <div style="flex: 0 0 250px; text-align: center; margin-bottom: 20px;">
    <img src="yifei2000.png" width="250">
    <p style="text-align: center; margin-top: 8px;">Original / Baseline (Iteration 2000)</p>
  </div>

  <div style="flex: 0 0 250px; text-align: center; margin-bottom: 20px;">
    <img src="L10W32.png" width="250">
    <p style="text-align: center; margin-top: 8px;">High PE L=10, Width=32</p>
  </div>

  <!-- Second row -->
  <div style="flex: 0 0 250px; text-align: center; margin-bottom: 20px;">
    <img src="L2W256.png" width="250">
    <p style="text-align: center; margin-top: 8px;">Low PE L=2, Width=256</p>
  </div>

  <div style="flex: 0 0 250px; text-align: center; margin-bottom: 20px;">
    <img src="L2W32.png" width="250">
    <p style="text-align: center; margin-top: 8px;">Low PE L=2, Width=32</p>
  </div>
</div>

<!-- PSNR curve description -->
<h3 style="text-align: center; margin-top: 40px;">PSNR Curve</h3>
<p>
  I also plotted the PSNR over training iterations for one image of my choice. The plot
  shows how reconstruction quality improves as training progresses. Higher positional
  encoding frequencies and wider networks converge faster and achieve higher PSNR values,
  while lower frequencies and narrower networks saturate at lower PSNR.
</p>

<div style="text-align: center; margin-top: 20px;">
  <img src="yifeipsnr_curve.png" width="500">
  <p style="text-align: center; margin-top: 8px;">PSNR over Training Iterations</p>
</div>

  
</div>

<!-- =========================== -->
<!-- Part 2 -->
<!-- =========================== -->
<div class="section" id="part2">
  <h2>Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>

  <p>
    In this part, we extend the neural field concept from Part 1 to a full 3D Neural Radiance Field (NeRF).
    Instead of a single 2D image, we now have multiple images of the object from different viewpoints, along
    with camera intrinsics and extrinsics. The goal is to represent the 3D scene and learn a continuous
    function that maps 3D points and viewing directions to RGB colors and densities.
  </p>

  <hr>

  <!-- =========================== -->
  <!-- Part 2.1 -->
  <!-- =========================== -->
  <h2 id="part2-1">Part 2.1: Create Rays from Cameras</h2>

  <h3>Camera to World Coordinate Conversion</h3>
  <p>
    The transformation between world coordinates <code>ùêó_w</code> and camera coordinates <code>ùêó_c</code>
    can be defined using a rotation matrix <code>R</code> and a translation vector <code>t</code>:
  </p>
  <p style="text-align: center;">
    $$\mathbf{X}_c = R \mathbf{X}_w + t$$
  </p>
  <p>
    The combined matrix is called the world-to-camera (w2c) transformation or extrinsic matrix. Its inverse
    gives the camera-to-world (c2w) transformation. In this session, we implement a function
    <code>x_w = transform(c2w, x_c)</code> that converts points from camera space to world space. We verify
    correctness using
  </p>
  <p style="text-align: center;">
    $$\mathbf{x} = \text{transform}(\text{c2w}^{-1}, \text{transform}(\text{c2w}, \mathbf{x}))$$
  </p>

  <h3>Pixel to Camera Coordinate Conversion</h3>
  <p>
    Consider a pinhole camera with intrinsic matrix
  </p>
  <p style="text-align: center;">
    $$K = \begin{bmatrix} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end{bmatrix}$$
  </p>
  <p>
    A 3D point in camera space <code>ùêó_c = [X_c, Y_c, Z_c]^T</code> can be projected to pixel coordinates <code>uv</code>:
  </p>
  <p style="text-align: center;">
    $$uv = K \frac{\mathbf{X}_c}{Z_c}$$
  </p>
  <p>
    We implement the inverse function <code>pixel_to_camera(K, uv, s)</code> to map a pixel coordinate back
    to a point in camera space. Both functions support batched coordinates for efficiency.
  </p>

  <h3>Pixel to Ray</h3>
  <p>
    Each pixel defines a ray with origin <code>r_o</code> and direction <code>r_d</code>. The camera origin
    in world space is given by the translation component of the <code>c2w</code> matrix. To compute the
    ray direction, we choose a point along the ray with depth 1 in camera coordinates and transform it to
    world coordinates. Then we normalize the vector to get <code>r_d</code>.
  </p>
  <p style="text-align: center; font-weight: bold; font-size: 18px;">
    $$\mathbf{x}(t) = \mathbf{r}_o + t \mathbf{r}_d$$
  </p>

  <hr>

  <!-- =========================== -->
  <!-- Part 2.2 -->
  <!-- =========================== -->
  <h2 id="part2-2">Part 2.2: Sampling</h2>

  <p>
    Once rays are defined for all pixels in all views, we sample a subset of rays for training. Two options
    exist: (1) sample a few images and then sample rays from each, or (2) flatten all pixels across all images
    and sample globally. Each ray consists of an origin and a direction, along with its corresponding
    RGB color from the image.
  </p>

  <h3>Sampling Points along Rays</h3>
  <p>
    To train NeRF, we discretize each ray into a set of 3D sample points. Uniformly sample along the ray between
    near and far bounds, for example, <code>near=2.0</code> and <code>far=6.0</code>. To reduce overfitting, we
    add a small random perturbation to each sample during training. The 3D coordinates for each point along the
    ray are then given by:
  </p>
  <p style="text-align: center; font-weight: bold; font-size: 18px;">
    $$\mathbf{x}(t) = \mathbf{r}_o + t \mathbf{r}_d$$
  </p>
  <p>
    Here, <code>n_samples</code> can be set to 32 or 64. These 3D points along the rays, together with their
    viewing directions, are used as input to the NeRF MLP for predicting RGB and density values.
  </p>
</div>

   <!-- =========================== -->
<!-- Part 2.3 -->
<!-- =========================== -->
<div class="section" id="part2-3">
  <h2>Part 2.3: Putting the Dataloading All Together</h2>

  <p>
    In this part, I implemented a dataloader that randomly samples pixels from all multi-view images.
    For each sampled pixel, I converted its 2D coordinates into a ray with origin and direction in world
    coordinates, and returned the corresponding RGB color. To verify that everything works correctly, I
    visualized the rays and sample points in 3D. I also tried sampling rays from a single camera to
    ensure that they lie inside the camera frustum and to catch any possible errors.
  </p>

  <!-- Example images -->
  <div style="display: flex; justify-content: center; gap: 30px; margin-top: 30px;">
    <!-- Image 1: Random 100 rays -->
    <div style="flex: 0 0 400px; text-align: center;">
      <img src="random_100_rays.png" alt="Random 100 Rays" width="400">
      <p>Random 100 Rays</p>
    </div>

    <!-- Image 2: 100 rays from one camera -->
    <div style="flex: 0 0 400px; text-align: center;">
      <img src="single_camera_100_rays.png" alt="100 Rays from One Camera" width="400">
      <p>100 Rays from One Camera</p>
    </div>
  </div>

</div>
<!-- =========================== -->
<!-- Part 2.4 -->
<!-- =========================== -->
<div class="section" id="part2_4">
  <h2>Part 2.4: Neural Radiance Field</h2>

  <p>
    After sampling 3D points along rays, the next step is to build the <b>Neural Radiance Field (NeRF)</b>.
    The network predicts both the <b>density</b> and <b>color</b> for each 3D point in world space.
    Compared with the 2D neural field in Part 1, there are several key changes:
  </p>

  <ul>
    <li>
      <b>Input:</b> 3D world coordinates of each sample point, along with the 3D ray direction.
      The view direction conditions the predicted color because the radiance depends on the viewing angle.
    </li>
    <li>
      <b>Output:</b> Density (œÉ) and color (RGB). Density is constrained to be non-negative using <code>ReLU</code>,
      while color is constrained to <code>[0,1]</code> with <code>Sigmoid</code>.
    </li>
    <li>
      <b>Positional Encoding:</b> Apply sinusoidal PE to both coordinates and ray directions.
      Coordinate PE can use a high frequency (e.g., L=10) while direction PE can use a lower frequency (e.g., L=4).
    </li>
    <li>
      <b>Deeper MLP:</b> Since modeling a 3D scene is more challenging, a deeper and wider MLP is used to improve capacity.
    </li>
    <li>
      <b>Input Injection:</b> After positional encoding, the input is also concatenated to the middle of the MLP.
      This helps the network retain spatial information and improves training stability.
    </li>
  </ul>

  <p>
    The network structure is illustrated below. You can start from this design and adjust depth, width, and
    PE frequencies based on your data and computational resources.
  </p>

  <div style="text-align: center; margin-top: 20px;">
    <img src="intronerf.png" width="700">
  </div>
</div>

<!-- =========================== -->
<!-- Part 2.5 -->
<!-- =========================== -->
<div class="section" id="part2-5">
  <h2>Part 2.5: Volume Rendering</h2>

  <p>
    In this part, I implemented the <b>volume rendering</b> equation for NeRF. The core idea
    is that along each ray, the color is computed by accumulating the contributions of all
    samples weighted by their density and transmittance. In continuous form:
  </p>

  <p style="text-align: center;">
    \( C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \, \sigma(\mathbf{r}(t)) \, \mathbf{c}(\mathbf{r}(t), \mathbf{d}) \, dt, \quad
    \text{where } T(t) = \exp\Big(-\int_{t_n}^t \sigma(\mathbf{r}(s)) ds\Big) \)
  </p>

  <p>
    For computation, I used the discrete approximation:
  </p>

  <p style="text-align: center;">
    \( \hat{C}(\mathbf{r}) = \sum_{i=1}^N T_i \left(1 - \exp(-\sigma_i \delta_i)\right) \mathbf{c}_i, \quad
    T_i = \exp\Big(-\sum_{j=1}^{i-1} \sigma_j \delta_j\Big) \)
  </p>

  <p>
    In practice, I implemented this in <b>PyTorch</b> to allow backpropagation. For each ray,
    I first sampled points along the ray, predicted their density (\(\sigma_i\)) and color (\(\mathbf{c}_i\))
    using the NeRF MLP, then applied the discrete volume rendering formula to get the final
    RGB value for that ray.
  </p>

  <p>
    I also visualized up to 100 rays and the corresponding sample points at a single training
    step to ensure the rays correctly lie within the camera frustum.
  </p>

  <!-- Process images row -->
  <div style="
      display: flex;
      justify-content: center;
      gap: 15px;
      margin-top: 20px;
  ">
    <div style="text-align: center;">
      <img src="lego100.png" width="160">
      <p style="margin-top: 6px;">Iteration 100</p>
    </div>

    <div style="text-align: center;">
      <img src="lego500.png" width="160">
      <p style="margin-top: 6px;">Iteration 500</p>
    </div>

    <div style="text-align: center;">
      <img src="lego1000.png" width="160">
      <p style="margin-top: 6px;">Iteration 1000</p>
    </div>

    <div style="text-align: center;">
      <img src="lego1500.png" width="160">
      <p style="margin-top: 6px;">Iteration 1500</p>
    </div>

    <div style="text-align: center;">
      <img src="lego2000.png" width="160">
      <p style="margin-top: 6px;">Iteration 2000</p>
    </div>
  </div>

  <!-- PSNR and Loss curves -->
  <p style="margin-top: 30px; text-align: center;">
    Below (left) shows the Validation PSNR Curve, which reaches 25 after 2000 iterations. 
    The training loss (right) curve goes down rapidly at the beginning and becomes stable in the later stage of training.
  </p>

  <div style="display: flex; justify-content: center; gap: 30px; margin-top: 10px;">
    <div style="text-align: center;">
      <img src="legoPSNR.png" width="400">
      <p style="margin-top: 6px;">Validation PSNR Curve</p>
    </div>

    <div style="text-align: center;">
      <img src="legoloss.png" width="400">
      <p style="margin-top: 6px;">Training Loss Curve</p>
    </div>
  </div>

  <!-- Spherical renderings GIF -->
  <p style="margin-top: 30px;">
    Here is the spherical renderings video of the Lego:
  </p>

  <div style="text-align: center; margin-top: 10px;">
    <img src="goodrendered.gif" width="400" loop="infinite">
  </div>

</div>

   
    
  </div>
</body>
</html>
