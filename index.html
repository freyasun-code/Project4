<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Project1: Images of the Russian Empire — Part 0</title>
  <style>
    body {
      background-color: #D8C3DD; /* 紫色背景（保持原来颜色） */
      font-family: Arial, sans-serif;
      color: white;
      margin: 0;
      padding: 0;
    }
    /* 导航栏（颜色与样式保持不变） */
    nav {
      background: rgba(0, 0, 0, 0.3);
      padding: 10px 20px;
      position: sticky;
      top: 0;
      z-index: 1000;
    }
    nav a {
      color: #ffddff;
      text-decoration: none;
      margin: 0 15px;
      font-weight: bold;
    }
    nav a:hover {
      text-decoration: underline;
    }
    .container {
      width: 80%;
      margin: auto;
      padding: 20px;
    }
    h1 {
      text-align: center;
      margin-top: 20px;
    }
    .section {
      background: rgba(0,0,0,0.2);
      padding: 20px;
      margin: 40px 0;
      border-radius: 12px;
    }
    .section h2 {
      color: #ffddff;
      text-align: center;
    }
    p {
      text-align: left;
      max-width: 1000px;
      margin: auto;
      margin-bottom: 20px;
      font-size: 18px;
      color: white;
    }
    hr {
      border: none;
      border-top: 1px solid rgba(255,255,255,0.12);
      margin: 30px 0;
    }
  </style>
  <!-- MathJax kept if needed later -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <nav>
    <a href="#part0">Part0</a>
    <a href="#part1">Part1</a>
    <a href="#part2">Part2</a>
    <a href="#part3">Part3</a>
  </nav>

  <div class="container">
    <h1>Project1: Images of the Russian Empire — Calibration & 3D Scan (Part 0)</h1>

    <!-- =========================== -->
    <!-- Part 0.0 -->
    <!-- =========================== -->
    <div class="section" id="part0">
      <h2>Part 0.0: Calibrating My Camera and Capturing a 3D Scan</h2>

      <h2 id="part0-1">Part 0.1: Calibrating My Camera</h2>

      <p>
        Following the assignment instructions, I first captured a set of calibration images
        containing multiple ArUco markers. I printed the calibration tag sheet and took
        around 40 images using my phone camera, keeping the zoom fixed while varying the
        viewing angle and distance. Phone cameras work well for this task since their images
        are already mostly undistorted.
      </p>

      <p>
        In my calibration script, I looped through all captured images and used OpenCV’s
        ArUco detector to locate the markers. For each detected tag, I extracted the 2D
        corner coordinates. Since the real-world physical size of each tag is known, I
        defined the 3D corner points accordingly (e.g., a 2cm × 2cm square mapped to
        four 3D points lying on the z=0 plane).
      </p>

      <p>
        After collecting all 2D–3D correspondences from all images, I used
        <code>cv2.calibrateCamera()</code> to estimate my camera intrinsics and distortion
        coefficients. These calibration parameters will later be used to compute object
        pose for the 3D scan.
      </p>

      <hr>

      <h2 id="part0-2">Part 0.2: Capturing the 3D Scan</h2>

      <p>
        With camera intrinsics estimated, I proceeded to capture image data for the 3D scan.
        I photographed the object from many viewpoints with substantial overlap (roughly 60–80%),
        maintaining consistent exposure and diffuse lighting to avoid harsh shadows. When possible,
        I placed the object on a simple turntable and rotated it incrementally; otherwise I walked
        the camera around the object while keeping the object centered in the frame.
      </p>

      <p>
        For robust reconstruction I captured images at multiple elevation angles (top, mid, low)
        and ensured good coverage of concave regions. I checked for motion blur and removed any
        blurry frames. All photos were saved in a single folder and named sequentially to preserve
        ordering.
      </p>

      <p>
        In post-processing I undistorted each image using the previously computed distortion
        coefficients, so the reconstruction algorithm receives geometrically-correct images.
        I then fed the undistorted image set and the camera intrinsics into a photogrammetry
        pipeline (e.g., COLMAP / OpenMVG + OpenMVS) to compute camera poses, sparse and dense
        point clouds, and finally a textured mesh (PLY/OBJ). The resulting 3D model can be
        aligned with the calibration coordinate frame so that measured 3D points correspond to
        the real-world scale used during calibration.
      </p>

      <p>
        This calibrated scan workflow ensures that the 3D geometry and the camera parameters are
        mutually consistent, which is important for later steps such as precise pose estimation,
        metric measurements, or combining multi-modal data.
      </p>
    </div>

  </div>
</body>
</html>
