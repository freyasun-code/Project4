<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Project4: Neural Radiance Field</title>
  <style>
    body {
      background-color: #D8C3DD; /* Á¥´Ëâ≤ËÉåÊôØ */
      font-family: Arial, sans-serif;
      color: white;
      margin: 0;
      padding: 0;
    }
    nav {
      background: rgba(0, 0, 0, 0.3);
      padding: 10px 20px;
      position: sticky;
      top: 0;
      z-index: 1000;
    }
    nav a {
      color: #ffddff;
      text-decoration: none;
      margin: 0 15px;
      font-weight: bold;
    }
    nav a:hover {
      text-decoration: underline;
    }
    .container {
      width: 80%;
      margin: auto;
      padding: 20px;
    }
    h1 {
      text-align: center;
      margin-top: 20px;
    }
    .section {
      background: rgba(0,0,0,0.2);
      padding: 20px;
      margin: 40px 0;
      border-radius: 12px;
    }
    .section h2 {
      color: #ffddff;
      text-align: center;
      margin-top: 0;
    }
    h3 {
      margin-top: 25px;
    }
    p {
      text-align: left;
      max-width: 1000px;
      margin: auto;
      margin-bottom: 20px;
      font-size: 18px;
      color: white;
    }
    hr {
      border: none;
      border-top: 1px solid rgba(255,255,255,0.12);
      margin: 30px 0;
    }
    .two-images {
      display: flex;
      justify-content: center;
      gap: 48px;
      margin-top: 30px;
      flex-wrap: wrap;
    }
    .img-block {
      flex: 0 0 320px;
      text-align: center;
    }
    .img-block img {
      width: 400px;
      border-radius: 8px;
      border: 1px solid rgba(255,255,255,0.08);
    }
    .img-block p {
      margin-top: 8px;
      font-size: 16px;
      color: white;
      text-align: center;
    }
    @media (max-width: 760px) {
      .container { width: 92%; padding: 12px; }
      .img-block { flex: 0 0 45%; }
      .img-block img { width: 100%; }
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <nav>
    <a href="#part0">Part0</a>
    <a href="#part1">Part1</a>
    <a href="#part2">Part2</a>
    <a href="#part3">Part3</a>
  </nav>

  <div class="container">
    <h1>Project4: Neural Radiance Field</h1>

    <!-- =========================== -->
    <!-- Part 0 -->
    <!-- =========================== -->
    <div class="section" id="part0">
      <h2>Part 0.0: Calibrating My Camera and Capturing a 3D Scan</h2>

      <h2 id="part0-1">Part 0.1: Calibrating My Camera</h2>
      <p>I first captured 33 calibration images containing multiple ArUco markers, keeping the zoom fixed while varying the viewing angle and distance.</p>
      <p>For each ArUco tag, I defined a local 3D coordinate system on the tag plane to represent its real-world coordinates. I then iterated over all calibration images and used OpenCV‚Äôs ArUco detector to find the 2D pixel coordinates of the detected tag corners. Images where no markers were detected were skipped.</p>
      <p>After collecting all 2D‚Äì3D correspondences across the dataset, I used cv2.calibrateCamera() to estimate the camera intrinsic matrix and distortion coefficients. These calibration parameters are subsequently used to compute object poses for 3D scanning.</p>
      <hr>

      <h2 id="part0-2">Part 0.2: Capturing the 3D Scan</h2>
      <p>I took 34 pictures of the bottle from many viewpoints with substantial overlap.</p>
    <div style="display: flex; justify-content: center; gap: 60px; margin-top: 20px;">
  <img src="bottle1.JPG" width="220">
  <img src="bottle2.JPG" width="220">
</div>


      <h2 id="part0-3">Part 0.3: Estimating Camera Pose</h2>
      <p>With the camera intrinsics calibrated in Part 0.1, I next estimated the camera's 3D position and orientation for each image. Each scan image contains a single ArUco tag placed next to the object. I used OpenCV‚Äôs ArUco detector to find the 2D pixel coordinates of the tag corners, and, knowing the tag‚Äôs physical size, defined the corresponding 3D coordinates on a flat z=0 plane. Using these 2D‚Äì3D correspondences, I applied <code>cv2.solvePnP()</code> to compute the rotation and translation of the tag relative to the camera. The resulting axis-angle vectors were converted to rotation matrices with <code>cv2.Rodrigues()</code>, and I inverted it to get the camera-to-world (c2w) poses needed for visualization and NeRF-style pipelines. After solving the extrinsics for all images, the camera poses and corresponding images are shown below.</p>

      <div class="two-images">
        <div class="img-block">
          <img src="pose1.jpg" alt="Pose Visualization 1">
          <p>Estimated Pose (Example 1)</p>
        </div>
        <div class="img-block">
          <img src="pose2.jpg" alt="Pose Visualization 2">
          <p>Estimated Pose (Example 2)</p>
        </div>
      </div>

      <h2 id="part0-4">Part 0.4: Undistorting Images and Building the Dataset</h2>
      <p>After estimating the pose for each image, I prepared the final dataset by undistorting all images using the calibration parameters. I used <code>cv2.undistort()</code> together with the calibrated intrinsics and distortion coefficients to obtain geometrically-correct images. Some images exhibit black borders after undistortion. I used <code>cv2.getOptimalNewCameraMatrix()</code> to compute a cropped intrinsics matrix and a valid ROI.</p>
      <p>I organized all undistorted images, camera intrinsics, and camera-to-world pose matrices into a structured dataset.</p>
    </div>

    <!-- =========================== -->
    <!-- Part 1 -->
    <!-- =========================== -->
    <div class="section" id="part1">
      <h2>Part 1: Fit a Neural Field to a 2D Image</h2>
      <p>In the 2D setting, the neural field learns a direct mapping from pixel coordinates <i>(x, y)</i> to RGB values. No volume rendering is involved.</p>

      <h3>1. Positional Encoding (PE)</h3>
      <p>Each 2D coordinate is expanded using sinusoidal positional encoding with <b>L = 10</b> frequency levels. This transforms the 2D input into a 42-D feature vector:</p>
      <p style="text-align:center;">
      $$
      \text{PE}(x) = \{ x, \ \sin(2^0 \pi x), \ \cos(2^0 \pi x), \ \dots, \ \sin(2^{L-1} \pi x), \ \cos(2^{L-1} \pi x) \}
      $$
      </p>
      <p>High-frequency PE allows the network to reproduce sharp edges and fine details.</p>

      <h3>2. MLP Architecture</h3>
      <p>The encoded coordinates are passed into a 4-layer MLP with 256 hidden units and ReLU activations. A final <code>Sigmoid</code> layer ensures RGB outputs fall within <i>[0, 1]</i>. The ground-truth image is also normalized to this range.</p>

      <h3>3. Random Pixel Sampling</h3>
      <p>Following the strategy used in my classmate‚Äôs implementation, each training iteration samples <b>10,000</b> random pixels:</p>
      <ul style="max-width:600px; margin:auto; text-align:left;">
        <li>The H√óW image is flattened into a 1-D array.</li>
        <li>Random indices are selected from {0 ‚Ä¶ H¬∑W‚àí1}.</li>
        <li>Each index <code>idx</code> is converted back to (x, y)</li>
        <li>Coordinates are normalized to [0, 1]: (x/W, y/H).</li>
        <li>The corresponding RGB values are retrieved for supervision.</li>
      </ul>

      <h2 style="margin-top:40px;">Training Results</h2>
      <p>I trained the network for 2000 iterations for each image. Initially, the model captures only a rough outline of the fox. As iterations increase, the reconstruction becomes clearer and recovers more fine-grained details. Eventually, the network produces a result that closely matches the original image.</p>

     <div style="display:flex; justify-content:center; gap:10px; flex-wrap:wrap; margin-top:30px;">
  <div style="text-align:center;">
    <img src="fox200.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 200</p>
  </div>
  <div style="text-align:center;">
    <img src="fox800.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 800</p>
  </div>
  <div style="text-align:center;">
    <img src="fox1400.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 1400</p>
  </div>
  <div style="text-align:center;">
    <img src="fox2000.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 2000</p>
  </div>
</div>

<h3 style="text-align:center; margin-top:40px;">My Own Image</h3>
<div style="display:flex; justify-content:center; gap:10px; flex-wrap:wrap; margin-top:20px;">
  <div style="text-align:center;">
    <img src="yifei200.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 200</p>
  </div>
  <div style="text-align:center;">
    <img src="yifei800.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 800</p>
  </div>
  <div style="text-align:center;">
    <img src="yifei1400.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 1400</p>
  </div>
  <div style="text-align:center;">
    <img src="yifei2000.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 2000</p>
  </div>
</div>



      <h3 style="text-align:center; margin-top:40px;">Final Results for Different Hyperparameters</h3>
      <p>I experimented with two choices of maximum positional encoding frequency (L) and two choices of MLP width. High frequency L = 10 and width = 256 captures fine details. Reducing width or frequency leads to blurrier results.</p>

      <div class="two-images">
        <div class="img-block"><img src="yifei2000.png" style="width:230px;"><p>L=10, Width=256</p></div>
        <div class="img-block"><img src="L10W32.png" style="width:230px;"><p>L=10, Width=32</p></div>
      </div>

      <div class="two-images" style="margin-bottom:20px;">
        <div class="img-block"><img src="L2W256.png" style="width:230px;"><p>L=2, Width=256</p></div>
        <div class="img-block"><img src="L2W32.png" style="width:230px;"><p>L=2, Width=32</p></div>
      </div>

      <h3 style="text-align:center; margin-top:40px;">PSNR Curve</h3>
      <p>I train the MLP using MSE with learning rate 1√ó10‚Åª¬≤. PSNR is computed as:</p>
      <p style="text-align:center;">$$\text{PSNR} = 10 \cdot \log_{10}\left(\frac{1}{\text{MSE}}\right)$$</p>
      <div class="two-images">
        <div class="img-block"><img src="yifeipsnr_curve.png" width="600"><p>PSNR over Training Iterations</p></div>
      </div>
    </div>


<!-- =========================== -->
<!-- Part 2 -->
<!-- =========================== -->
<div class="section" id="part2">
  <h2>Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>

  <!-- =========================== -->
  <!-- Part 2.1 -->
  <!-- =========================== -->
    <h2>Part 2.1: Create Rays from Cameras</h2>

    <h3>Camera to World Coordinate Conversion</h3>
    <p>
      The transformation between world coordinates <code>ùêó_w</code> and camera coordinates <code>ùêó_c</code>
      can be defined using a rotation matrix <code>R</code> and a translation vector <code>t</code>:
    </p>
    <p style="text-align: center;">
      $$\mathbf{X}_c = R \mathbf{X}_w + t$$
    </p>
    <p>
      The combined matrix is called the world-to-camera (w2c) transformation or extrinsic matrix. Its inverse
      gives the camera-to-world (c2w) transformation.
    </p>

    <h3>Pixel to Camera Coordinate Conversion</h3>
    <p>
      Consider a pinhole camera with intrinsic matrix
    </p>
    <p style="text-align: center;">
      $$K = \begin{bmatrix} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end{bmatrix}$$
    </p>
    <p>
      A 3D point in camera space <code>ùêó_c = [X_c, Y_c, Z_c]^T</code> can be projected to pixel coordinates <code>uv</code>:
    </p>
    <p style="text-align: center;">
      $$uv = K \frac{\mathbf{X}_c}{Z_c}$$
    </p>

    <h3>Pixel to Ray</h3>
    <p>
      Each pixel defines a ray with origin <code>r_o</code> and direction <code>r_d</code>. The camera origin
      in world space is given by the translation component of the <code>c2w</code> matrix. The ray direction is normalized:
    </p>
    <p style="text-align: center; font-weight: bold; font-size: 18px;">
      $$\mathbf{x}(t) = \mathbf{r}_o + t \mathbf{r}_d$$
    </p>


  <!-- =========================== -->
  <!-- Part 2.2 -->
  <!-- =========================== -->
  <h2 id="part2-2">Part 2.2: Sampling</h2>

  <p>
    Once rays are defined for all pixels in all views, we sample a subset of rays for training. 
    Each ray consists of an origin and a direction, along with its corresponding RGB color from the image.
  </p>

  <h3>Sampling Points along Rays</h3>
  <p>
    To train NeRF, each ray is discretized into a set of 3D sample points along its path. 
    For each ray, define near and far bounds and split the segment into <code>n_samples</code> bins.
  </p>

  <!-- =========================== -->
  <!-- Part 2.3 -->
  <!-- =========================== -->
  <h2 id="part2-3">Part 2.3: Dataloader</h2>

  <p>
    Implemented a dataloader that randomly samples pixels from all multi-view images.
    Each sampled pixel is converted to a ray with origin and direction in world coordinates,
    and the corresponding RGB color is returned.
  </p>

  <div class="two-images" style="margin-top:20px;">
    <div class="img-block">
      <img src="random_100_rays.png" alt="Random 100 Rays">
      <p>Random 100 Rays</p>
    </div>
    <div class="img-block">
      <img src="single_camera_100_rays.png" alt="100 Rays from One Camera">
      <p>100 Rays from One Camera</p>
    </div>
  </div>

  <!-- =========================== -->
  <!-- Part 2.4 -->
  <!-- =========================== -->
  <h2 id="part2-4">Part 2.4: Neural Radiance Field</h2>

  <p>
    Build the <b>Neural Radiance Field (NeRF)</b>. The network predicts both <b>density</b> and <b>color</b> for each 3D point.
  </p>

  <ul>
    <li><b>Input:</b> 3D world coordinates and 3D ray directions.</li>
    <li><b>Output:</b> Density (œÉ) and RGB color. Density via ReLU, color via Sigmoid.</li>
    <li><b>Positional Encoding:</b> Sinusoidal PE for coordinates (L=10) and directions (L=4).</li>
    <li><b>Input Injection:</b> PE inputs concatenated to middle of MLP for stability.</li>
  </ul>

  <div style="text-align: center; margin-top: 20px;">
    <img src="intronerf.png" width="700" alt="NeRF Architecture">
  </div>

  <!-- =========================== -->
  <!-- Part 2.5 -->
  <!-- =========================== -->
  <h2 id="part2-5">Part 2.5: Volume Rendering</h2>

  <p>
    Implemented the volume rendering equation. Continuous form:
  </p>
  <p style="text-align:center;">
    \( C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \, \sigma(\mathbf{r}(t)) \, \mathbf{c}(\mathbf{r}(t), \mathbf{d}) \, dt, \quad
    T(t) = \exp\Big(-\int_{t_n}^t \sigma(\mathbf{r}(s)) ds\Big) \)
  </p>

  <p>
    Discrete approximation:
  </p>
  <p style="text-align:center;">
    \( \hat{C}(\mathbf{r}) = \sum_{i=1}^N T_i (1 - \exp(-\sigma_i \delta_i)) \mathbf{c}_i, \quad
    T_i = \exp\Big(-\sum_{j=1}^{i-1} \sigma_j \delta_j\Big) \)
  </p>

  <p>
    PyTorch implementation with 2000 iterations, 4096 rays per iteration, 64 points per ray.
    Network width 256, PE levels 10 for points and 4 for directions, Adam optimizer LR=5e-4.
  </p>

  <!-- Five intermediate images in a row -->
<div style="display:flex; justify-content:center; gap:10px; flex-wrap:wrap; margin-top:20px;">
  <div style="text-align:center;">
    <img src="lego100.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 100</p>
  </div>
  <div style="text-align:center;">
    <img src="lego500.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 500</p>
  </div>
  <div style="text-align:center;">
    <img src="lego1000.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 1000</p>
  </div>
  <div style="text-align:center;">
    <img src="lego1500.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 1500</p>
  </div>
  <div style="text-align:center;">
    <img src="lego2000.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 2000</p>
  </div>
</div>



  <div class="two-images" style="margin-top:30px;">
    <div class="img-block"><img src="legoPSNR.png" width="350"><p>Validation PSNR Curve</p></div>
    <div class="img-block"><img src="legoloss.png" width="350"><p>Training Loss Curve</p></div>
  </div>

  <div style="text-align:center; margin-top:30px;">
    <img src="loopedle.gif" width="400"><p>Rendered GIF (Orbit View)</p>
  </div>

  <!-- =========================== -->
  <!-- Part 2.6 -->
  <!-- =========================== -->
  <h2 id="part2-6">Part 2.6: Training with Your Own Data</h2>

  <p>
    For my bottle dataset, training settings: 5000 iterations, 10,000 rays per iteration, 64 points per ray,
    near/far bounds: 0.03 / 0.5. Network width 256, PE 10 for points, 4 for directions, Adam LR=5e-4.
  </p>

  <!-- Five intermediate images in a row -->
<div style="display:flex; justify-content:center; gap:10px; flex-wrap:wrap; margin-top:20px;">
  <div style="text-align:center;">
    <img src="bottle1000.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 1000</p>
  </div>
  <div style="text-align:center;">
    <img src="bottle2000.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 2000</p>
  </div>
  <div style="text-align:center;">
    <img src="bottle3000.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 3000</p>
  </div>
  <div style="text-align:center;">
    <img src="bottle4000.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 4000</p>
  </div>
  <div style="text-align:center;">
    <img src="bottle5000.png" width="170" style="display:block; margin:auto;">
    <p style="margin-top:8px;">Iteration 5000 (Final)</p>
  </div>
</div>



  <div class="two-images" style="margin-top:30px;">
    <div class="img-block"><img src="bottlepsnr.png" width="340"><p>Validation PSNR Curve</p></div>
    <div class="img-block"><img src="bottleloss.png" width="340"><p>Training Loss Curve</p></div>
  </div>

  <div style="text-align:center; margin-top:40px;">
    <img src="bottle.gif" style="width:360px;"><p>Final Rendering (Orbit View)</p>
  </div>
  <p>
    You can see there‚Äôs a yellow cup next to the marker. At first, I set the <code>far</code> value too large, 
    which made the training results look overexposed. I realized that my cup is actually quite small, 
    so I reduced the <code>far</code> value. 
  </p>

  <p>
    Also, I made a serious mistake: my images were automatically rotated 90 degrees in Jupyter, 
    which caused my initial renders to always look wrong.
  </p>
</div>

  
</div>


  </div>
</body>
</html>
