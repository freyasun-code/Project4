<!DOCTYPE html>

<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Project 3B: Camera Calibration and 3D Scanning</title>
  
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['\\(','\\)']],
        displayMath: [['\\[','\\]']]
      },
      options: {
        skipHtmlTags: ['script','noscript','style','textarea','pre','code']
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
  <style>
    body {
      background-color: #D8C3DD;
      font-family: Arial, sans-serif;
      color: white; /* 页面通用文字色，用于 h1, h2, nav 等 */
      margin: 0;
      padding: 0;
    }
    nav {
      background: rgba(0, 0, 0, 0.3);
      padding: 10px 20px;
      position: sticky;
      top: 0;
      z-index: 1000;
    }
    nav a {
      color: #ffddff;
      text-decoration: none;
      margin: 0 15px;
      font-weight: bold;
    }
    nav a:hover {
      text-decoration: underline;
    }
    .container {
      width: 90%;
      margin: auto;
      padding: 20px;
    }
    h1 {
      text-align: center;
      margin-top: 20px;
    }
    .section {
      background-color: #D7D3EB; /* 固定浅紫灰色，不透明 */
      padding: 30px;
      margin: 40px 0;
      border-radius: 12px;
      color: #333; /* 修正：确保 section 内正文为深色，可见 */
    }
    .section h2 {
      color: #ffddff; /* 标题颜色保持不变 */
      text-align: center;
    }
    .formula {
      display: flex;
      justify-content: center;
      align-items: center;
      text-align: center;
      font-style: italic;
      font-size: 1.05em;
      margin: 12px 0;
      line-height: 1.5;
      color: #000; /* 确保公式文字可见 */
    }
    img {
      max-width: 100%;
      border-radius: 8px;
    }
    .row {
      display: flex;
      justify-content: center;
      gap: 30px;
      margin: 20px 0;
      flex-wrap: wrap;
    }
    .img-block {
      text-align: center;
    }
    .img-block p {
      margin-top: 8px;
      font-size: 16px;
      text-align: center;
      color: #333; /* 确保图片说明文字可见 */
    }
    p {
      text-align: left;
      max-width: 1000px;
      margin: auto;
      margin-bottom: 20px;
      font-size: 18px;
      color: #333; /* 确保段落文字可见 */
    }
    code {
      background: #f2f2f2;
      padding: 3px 5px;
      border-radius: 4px;
      color: #000000; /* 确保代码块内的文字可见 */
      font-family: monospace;
    }
  </style>
</head>

<body>
  <nav>
        <a href="#P00">P 0.0</a>
    <a href="#P01">P 0.1</a>
    <a href="#P02">P 0.2</a>
    <a href="#P03">P 0.3</a>
  </nav>

  <div class="container">
    <h1>Project 3B: Camera Calibration and 3D Scanning</h1>
    <p style="text-align:center; font-style:italic; color: white;">Author Name</p>

<div class="section" id="P00">
  <h2>Part 0.0: Project Overview</h2>
  <p>
    This project focuses on **Camera Calibration** using ArUco markers and its application in **3D Scene Reconstruction (Scanning)**. We first estimate the camera's intrinsic parameters and distortion coefficients using multiple views of a known pattern. Then, we leverage these parameters to determine the 3D pose of objects in the scene from 2D image coordinates.
  </p>
</div>

<div class="section" id="P01">
  <h2>Part 0.1: Calibrating My Camera</h2>
  <p>
    Following the assignment instructions, I first captured a set of calibration images
    containing multiple ArUco markers. I printed the calibration tag sheet and took
    around 40 images using my phone camera, keeping the zoom fixed while varying the
    viewing angle and distance. Phone cameras work well for this task since their images
    are already mostly undistorted.
  </p>

  <p>
    In my calibration script, I looped through all captured images and used OpenCV’s
    ArUco detector to locate the markers. For each detected tag, I extracted the 2D
    corner coordinates. Since the real-world physical size of each tag is known, I
    defined the 3D corner points accordingly (e.g., a 2cm × 2cm square mapped to
    four 3D points lying on the $z=0$ plane).
  </p>

  <p>
    After collecting all 2D–3D correspondences from all images, I used
    <code>cv2.calibrateCamera()</code> to estimate my camera intrinsics and distortion
    coefficients. These calibration parameters will later be used to compute object
    pose for the 3D scan.
  </p>
  
  <div class="row">
    <div class="img-block">
      <img src="calibration-setup-image.jpg" alt="Calibration setup" width="450">
      <p>Calibration setup with ArUco markers.</p>
    </div>
    <div class="img-block">
      <img src="detected-markers-image.jpg" alt="Detected markers" width="450">
      <p>Visualization of detected corner points.</p>
    </div>
  </div>
</div>


<div class="section" id="P02">
  <h2>Part 0.2: Capturing a 3D Scan</h2>

  <p>
    To capture the 3D scan, a fixed object (e.g., a simple cube or a 3D printed model) was placed on the plane containing the ArUco markers. Multiple images were taken from different viewpoints around the object.
  </p>

  <p>
    For each image, the known ArUco marker pattern allows us to robustly determine the camera's **Extrinsic Parameters** (Rotation $\mathbf{R}$ and Translation $\mathbf{T}$) relative to the world coordinate system (the marker plane). This is achieved using the calibrated intrinsic matrix $\mathbf{K}$:
  </p>

  <div class="formula">
    \[
    \lambda \mathbf{x} = \mathbf{K} [\mathbf{R} | \mathbf{T}] \mathbf{X}
    \]
  </div>

  <p>
    where $\mathbf{x}$ is a 2D image point and $\mathbf{X}$ is a 3D world point. By projecting the known marker points and minimizing the re-projection error, we obtain the precise camera pose. These poses are essential for triangulating the 3D coordinates of points on the object.
  </p>

  <div class="row">
    <div class="img-block">
      <img src="object-scan-view1.jpg" alt="3D scan view 1" width="300">
      <p>Object View 1 with Pose determined.</p>
    </div>
    <div class="img-block">
      <img src="object-scan-view2.jpg" alt="3D scan view 2" width="300">
      <p>Object View 2 with Pose determined.</p>
    </div>
    <div class="img-block">
      <img src="object-scan-view3.jpg" alt="3D scan view 3" width="300">
      <p>Object View 3 with Pose determined.</p>
    </div>
  </div>
</div>

<div class="section" id="P03">
  <h2>Part 0.3: Result Visualization and Discussion</h2>

  <p>
    Using a Structure-from-Motion (SfM) or similar pipeline, the 2D correspondences across the captured images were found, and their 3D locations were triangulated using the known camera poses. This process yields a sparse 3D point cloud of the scanned object.
  </p>

  <p>
    The final visualization shows the resulting point cloud, clearly illustrating the shape of the object. While the density is limited by the number of feature correspondences, the overall structure demonstrates the effectiveness of the camera calibration and pose estimation approach.
  </p>
  
  <div class="row">
    <div class="img-block">
      <img src="final-point-cloud-visualization.jpg" alt="Final 3D point cloud" width="600">
      <p>The final sparse 3D point cloud of the scanned object.</p>
    </div>
  </div>
</div>


  </div>
</body>
</html>
