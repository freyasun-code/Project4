<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Project Webpage</title>

  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f9f9f9;
      line-height: 1.6;
    }

    .container {
      width: 80%;
      margin: 40px auto;
      background: white;
      padding: 40px;
      border-radius: 10px;
      box-shadow: 0 0 15px #ddd;
    }

    h1, h2 {
      color: #333;
      text-align: left;
      margin-top: 40px;
    }

    p, ul, li, ol {
      color: #444;
      font-size: 16px;
    }

    img {
      display: block;
      margin: 20px auto;
      max-width: 90%;
      border-radius: 6px;
    }

    hr {
      margin: 40px 0;
      border: none;
      border-top: 1px solid #ccc;
    }
  </style>
</head>

<body>

  <div class="container">

    <!-- ======================================================= -->
    <!-- A.1: Homographies -->
    <!-- ======================================================= -->
    <h2>A.1: Homographies</h2>

    <p>
      In this part, I implemented homography estimation using manually selected point correspondences.
      Four pairs of matching points are used to solve for the homography matrix
      using the linear DLT algorithm.
    </p>

    <h3>Rectification Example</h3>
    <p>I selected four points on a rectangular surface in the image and computed the homography to warp it into a fronto-parallel view.</p>
    <img src="a1_rectify_before.png">
    <img src="a1_rectify_after.png">

    <hr>

    <!-- ======================================================= -->
    <!-- A.2: Recovering the Homography -->
    <!-- ======================================================= -->
    <h2>A.2: Recovering the Homography</h2>

    <p>
      In this section, I computed homographies for image mosaicing. I selected 4–6 point correspondences manually
      between the two images and solved for the homography that maps one image into the coordinate frame of the other.
    </p>

    <img src="a2_points.png">
    <img src="a2_mosaic.png">

    <hr>

    <!-- ======================================================= -->
    <!-- Part 0.1: Calibrating My Camera -->
    <!-- ======================================================= -->
    <h2>Part 0.1: Calibrating My Camera</h2>

    <p>
      I printed the provided calibration ArUco tag sheet and captured around 40 images using my phone camera,
      keeping the zoom fixed while varying angles and distances. This gives OpenCV enough diverse views 
      for accurate calibration.
    </p>

    <p>
      I used OpenCV’s ArUco detector to locate tag corners in each image. The physical size of the tag is known,
      so I assigned corresponding 3D corner coordinates on the z=0 plane. After collecting all 2D–3D correspondences,
      I used <code>cv2.calibrateCamera()</code> to compute intrinsics and distortion parameters.
    </p>

    <img src="placeholder_setup.png">

    <hr>

    <!-- ======================================================= -->
    <!-- Part 0.2: Capturing the 3D Scan -->
    <!-- ======================================================= -->
    <h2>Part 0.2: Capturing a 3D Object Scan</h2>

    <p>
      After calibration, I printed a single ArUco tag and placed it next to my object on a table. 
      I captured ~40 images from different viewpoints, keeping the same camera device and zoom level.
      I avoided blurry or over-exposed frames and kept a roughly constant distance so the object 
      occupied ~50% of the frame.
    </p>

    <p>
      Before pose estimation, I uniformly downsampled all images for efficiency. 
      I kept the aspect ratio unchanged and displayed the downsampled images to ensure the orientation
      and dimensions were correct. As instructed, I did not perform undistortion or cropping.
    </p>

    <img src="placeholder_dataflow.png">

    <hr>

  </div> <!-- container -->

</body>
</html>
