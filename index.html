<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Project4: Neural Radiance Field</title>
  <style>
    body {
      background-color: #D8C3DD; /* Á¥´Ëâ≤ËÉåÊôØÔºà‰øùÊåÅÂéüÊù•È¢úËâ≤Ôºâ */
      font-family: Arial, sans-serif;
      color: white;
      margin: 0;
      padding: 0;
    }
    /* ÂØºËà™Ê†èÔºàÈ¢úËâ≤‰∏éÊ†∑Âºè‰øùÊåÅ‰∏çÂèòÔºâ */
    nav {
      background: rgba(0, 0, 0, 0.3);
      padding: 10px 20px;
      position: sticky;
      top: 0;
      z-index: 1000;
    }
    nav a {
      color: #ffddff;
      text-decoration: none;
      margin: 0 15px;
      font-weight: bold;
    }
    nav a:hover {
      text-decoration: underline;
    }
    .container {
      width: 80%;
      margin: auto;
      padding: 20px;
    }
    h1 {
      text-align: center;
      margin-top: 20px;
    }
    .section {
      background: rgba(0,0,0,0.2);
      padding: 20px;
      margin: 40px 0;
      border-radius: 12px;
    }
    .section h2 {
      color: #ffddff;
      text-align: center;
      margin-top: 0;
    }
    p {
      text-align: left;
      max-width: 1000px;
      margin: auto;
      margin-bottom: 20px;
      font-size: 18px;
      color: white;
    }
    hr {
      border: none;
      border-top: 1px solid rgba(255,255,255,0.12);
      margin: 30px 0;
    }

    /* ÂõæÁâáÊéíÁâàÂæÆË∞ÉÔºàÊõ¥Â§ß„ÄÅÊõ¥Â±Ö‰∏≠„ÄÅÈó¥Ë∑ùÊõ¥Â§ßÔºâ */
    .two-images {
      display: flex;
      justify-content: center;
      gap: 48px; /* ‰∏≠Èó¥Á©∫‰∏ÄÁÇπ */
      margin-top: 30px;
      flex-wrap: wrap;
    }
    .img-block {
      flex: 0 0 320px; /* Âü∫ÂáÜÊõ¥Â§ß */
      text-align: center;
    }
    .img-block img {
      width: 400px; /* ÊîæÂ§ßÂõæÁâá */
      border-radius: 8px;
      border: 1px solid rgba(255,255,255,0.08);
    }
    .img-block p {
      margin-top: 8px;
      font-size: 16px;
      color: white;
      text-align: center;
    }

    /* ‰øùËØÅÂ∞èÂ±èÂπïÈÄÇÈÖç */
    @media (max-width: 760px) {
      .container { width: 92%; padding: 12px; }
      .img-block { flex: 0 0 45%; }
      .img-block img { width: 100%; }
    }
  </style>
  <!-- MathJax kept if needed later -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <nav>
    <a href="#part0">Part0</a>
    <a href="#part1">Part1</a>
    <a href="#part2">Part2</a>
    <a href="#part3">Part3</a>
  </nav>

  <div class="container">
    <h1>Project1: Images of the Russian Empire ‚Äî Calibration & 3D Scan (Part 0)</h1>

    <!-- =========================== -->
    <!-- Part 0.0 -->
    <!-- =========================== -->
    <div class="section" id="part0">
      <h2>Part 0.0: Calibrating My Camera and Capturing a 3D Scan</h2>

      <h2 id="part0-1">Part 0.1: Calibrating My Camera</h2>

      <p>
        I first captured 33 calibration images
        containing multiple ArUco markers, keeping the zoom fixed while varying the
        viewing angle and distance.
      </p>

      <p>
        For each ArUco tag, I defined a local 3D coordinate system on the tag plane to represent its real-world coordinates. 
        I then iterated over all calibration images and used OpenCV‚Äôs ArUco detector to find the 2D pixel coordinates of the detected tag corners. 
        Images where no markers were detected were skipped.
      </p>

      <p>
        After collecting all 2D‚Äì3D correspondences across the dataset, I used cv2.calibrateCamera() to estimate the camera intrinsic matrix and distortion coefficients. 
        These calibration parameters are subsequently used to compute object poses for 3D scanning.
      </p>

      <hr>

      <h2 id="part0-2">Part 0.2: Capturing the 3D Scan</h2>

      <p>
          I took 34 pictures of the bottle from many viewpoints with substantial overlap.
      </p>
      
      <p>
          <img src="bottle1.JPG" width="250" style="margin-right: 20px;">
          <img src="bottle2.JPG" width="250">
      </p>

    </div>

    <!-- =========================== -->
    <!-- Part 0.3 -->
    <!-- =========================== -->
    <div class="section" id="part0-3-section">
      <h2 id="part0-3">Part 0.3: Estimating Camera Pose</h2>

      <p>
      
With the camera intrinsics calibrated in Part 0.1, I next estimated the camera's 3D position and orientation for each image.
        Each scan image contains a single ArUco tag placed next to the object. 
        I used OpenCV‚Äôs ArUco detector to find the 2D pixel coordinates of the tag corners, 
        and, knowing the tag‚Äôs physical size, defined the corresponding 3D coordinates on a flat z=0 plane. 
        Using these 2D‚Äì3D correspondences, I applied <code>cv2.solvePnP()</code> to compute the rotation and translation of the tag relative to the camera. 
        The resulting axis-angle vectors were converted to rotation matrices with <code>cv2.Rodrigues()</code>, 
        and I inverted it to get the camera-to-world (c2w) poses needed for visualization and NeRF-style pipelines. 
        After solving the extrinsics for all images, the camera poses and corresponding images are shown below.
</p>

      <!-- Example images for Part 0.3 (two images, centered, larger) -->
      <div class="two-images" role="group" aria-label="Pose examples">
        <div class="img-block">
          <img src="pose1.jpg" alt="Pose Visualization 1">
          <p>Estimated Pose (Example 1)</p>
        </div>

        <div class="img-block">
          <img src="pose2.jpg" alt="Pose Visualization 2">
          <p>Estimated Pose (Example 2)</p>
        </div>
      </div>
    </div>


    <!-- =========================== -->
    <!-- Part 0.4 -->
    <!-- =========================== -->
    <div class="section" id="part0-4-section">
      <h2 id="part0-4">Part 0.4: Undistorting Images and Building the Dataset</h2>

      <p>
        After estimating the pose for each image, I prepared the final dataset by undistorting all
        images using the calibration parameters.
        I used <code>cv2.undistort()</code> together with the calibrated intrinsics and distortion 
        coefficients to obtain geometrically-correct images. Some images exhibit black borders 
        after undistortion. I used 
        <code>cv2.getOptimalNewCameraMatrix()</code> to compute a cropped intrinsics matrix and a valid ROI.
      </p>

      <p>
        I organized all undistorted images, camera intrinsics, and camera-to-world pose matrices
        into a structured dataset.  
      </p>
    </div>

<!-- =========================== -->
<!-- Part 1 -->
<!-- =========================== -->
<div class="section" id="part1" style="background: rgba(0,0,0,0.2); padding: 20px; border-radius: 12px;">
  <h2 style="color:#ffddff; text-align:center;">Part 1: Fit a Neural Field to a 2D Image</h2>

  <p>
    In the 2D setting, the neural field learns a direct mapping from pixel coordinates 
    <i>(x, y)</i> to RGB values. No volume rendering is involved.
  </p>

  <h3>1. Positional Encoding (PE)</h3>
  <p>
    Each 2D coordinate is expanded using sinusoidal positional encoding with 
    <b>L = 10</b> frequency levels. This transforms the 2D input into a 42-D feature vector:
  </p>

  <div style="text-align:center; margin:10px 0;">
    PE(x) = [x, sin(2<sup>0</sup>œÄx), cos(2<sup>0</sup>œÄx), ‚Ä¶, sin(2<sup>L‚àí1</sup>œÄx), cos(2<sup>L‚àí1</sup>œÄx)]
  </div>

  <p>
    High-frequency PE allows the network to reproduce sharp edges and fine details.
  </p>

  <h3>2. MLP Architecture</h3>
  <p>
    The encoded coordinates are passed into a 4-layer MLP with 256 hidden units and ReLU activations.
    A final <code>Sigmoid</code> layer ensures RGB outputs fall within <i>[0, 1]</i>. 
    The ground-truth image is also normalized to this range.
  </p>

  <h3>3. Random Pixel Sampling</h3>
  <p>
    Following the strategy used in my classmate‚Äôs implementation, each training iteration samples 
    <b>10,000</b> random pixels:
  </p>

  <ul style="max-width:600px; margin:auto; text-align:left;">
    <li>The H√óW image is flattened into a 1-D array.</li>
    <li>Random indices are selected from {0 ‚Ä¶ H¬∑W‚àí1}.</li>
    <li>Each index <code>idx</code> is converted back to (x, y)</li>
    <li>Coordinates are normalized to [0, 1]: (x/W, y/H).</li>
    <li>The corresponding RGB values are retrieved for supervision.</li>
  </ul>

  <!-- =========================== -->
  <!-- Image row (4 images) -->
  <!-- =========================== -->
  <div style="display:flex; justify-content:center; gap:30px; flex-wrap:wrap; margin-top:30px;">
    <div style="text-align:center;">
      <img src="fox200.png" width="200" style="display:block; margin:auto;">
      <p style="margin-top:8px;">Iteration 200</p>
    </div>
    <div style="text-align:center;">
      <img src="fox800.png" width="200" style="display:block; margin:auto;">
      <p style="margin-top:8px;">Iteration 800</p>
    </div>
    <div style="text-align:center;">
      <img src="fox1400.png" width="200" style="display:block; margin:auto;">
      <p style="margin-top:8px;">Iteration 1400</p>
    </div>
    <div style="text-align:center;">
      <img src="fox2000.png" width="200" style="display:block; margin:auto;">
      <p style="margin-top:8px;">Iteration 2000</p>
    </div>
  </div>

  <!-- My own image row -->
  <h3 style="text-align:center; margin-top:40px;">My own image</h3>
  <div style="display:flex; justify-content:center; gap:30px; flex-wrap:wrap; margin-top:20px;">
    <div style="text-align:center;">
      <img src="yifei200.png" width="200" style="display:block; margin:auto;">
      <p style="margin-top:8px;">Iteration 200</p>
    </div>
    <div style="text-align:center;">
      <img src="yifei800.png" width="200" style="display:block; margin:auto;">
      <p style="margin-top:8px;">Iteration 800</p>
    </div>
    <div style="text-align:center;">
      <img src="yifei1400.png" width="200" style="display:block; margin:auto;">
      <p style="margin-top:8px;">Iteration 1400</p>
    </div>
    <div style="text-align:center;">
      <img src="yifei2000.png" width="200" style="display:block; margin:auto;">
      <p style="margin-top:8px;">Iteration 2000</p>
    </div>
  </div>


<!-- =========================== -->
<!-- My own image -->
<!-- =========================== -->
<h3 style="text-align: center; margin-top: 40px;">My own image</h3>

<div style="
    display: flex;
    justify-content: center;
    gap: 30px;
    margin-top: 20px;
">
  <div style="text-align: center;">
    <img src="yifei200.png" width="200">
    <p style="margin-top: 8px;">Iteration 200</p>
  </div>

  <div style="text-align: center;">
    <img src="yifei800.png" width="200">
    <p style="margin-top: 8px;">Iteration 800</p>
  </div>

  <div style="text-align: center;">
    <img src="yifei1400.png" width="200">
    <p style="margin-top: 8px;">Iteration 1400</p>
  </div>

  <div style="text-align: center;">
    <img src="yifei2000.png" width="200">
    <p style="margin-top: 8px;">Iteration 2000</p>
  </div>
</div>
<!-- =========================== -->
<!-- Final Results / Hyperparameter Tuning -->
<!-- =========================== -->
<h3 style="text-align: center; margin-top: 40px;">Final Results for Different Hyperparameters</h3>

<p>
  I experimented with two choices of maximum positional encoding frequency (L) and two
  choices of MLP width (number of hidden channels). With a high frequency L = 10 and
  a wide network (width = 256), the network captures fine details and sharp edges very well.
  When the width is reduced to 32 while keeping L = 10, the main structures are still preserved
  but the output becomes slightly noisier and blurrier. Lowering the frequency to L = 2 makes
  the network only capable of representing smooth, low-frequency patterns, and reducing
  width to 32 further exaggerates this smoothing.
</p>

<!-- First row -->
<div style="display: flex; justify-content: center; gap: 40px; margin-top: 20px; margin-bottom: 20px;">

  <div style="text-align: center;">
    <img src="yifei2000.png" width="250">
    <p style="margin-top: 8px;">Original / Baseline (Iteration 2000)</p>
  </div>

  <div style="text-align: center;">
    <img src="L10W32.png" width="250">
    <p style="margin-top: 8px;">High PE L=10, Width=32</p>
  </div>

</div>

<!-- Second row -->
<div style="display: flex; justify-content: center; gap: 40px; margin-bottom: 20px;">

  <div style="text-align: center;">
    <img src="L2W256.png" width="250">
    <p style="margin-top: 8px;">Low PE L=2, Width=256</p>
  </div>

  <div style="text-align: center;">
    <img src="L2W32.png" width="250">
    <p style="margin-top: 8px;">Low PE L=2, Width=32</p>
  </div>

</div>


<!-- PSNR curve description -->
<h3 style="text-align: center; margin-top: 40px;">PSNR Curve</h3>

<p>
  I train the MLP using mean squared error (MSE) and the Adam optimizer (learning rate 1√ó10‚Åª¬≤).
  To monitor reconstruction quality, I compute the peak signal-to-noise ratio (PSNR):
</p>

<p style="margin-left:20px;">
  PSNR = 10 ¬∑ log10(1 / MSE)
</p>
<p>
  I also plotted the PSNR over training iterations for one image of my choice. The plot
  shows how reconstruction quality improves as training progresses. Higher positional
  encoding frequencies and wider networks converge faster and achieve higher PSNR values,
  while lower frequencies and narrower networks saturate at lower PSNR.
</p>

<div style="text-align: center; margin-top: 20px;">
  <img src="yifeipsnr_curve.png" width="500">
  <p style="text-align: center; margin-top: 8px;">PSNR over Training Iterations</p>
</div>

  
</div>

<!-- =========================== -->
<!-- Part 2 -->
<!-- =========================== -->
<div class="section" id="part2">
  <h2>Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>

  <p>
    In this part, we extend the neural field concept from Part 1 to a full 3D Neural Radiance Field (NeRF).
    Instead of a single 2D image, we now have multiple images of the object from different viewpoints, along
    with camera intrinsics and extrinsics. The goal is to represent the 3D scene and learn a continuous
    function that maps 3D points and viewing directions to RGB colors and densities.
  </p>

  <hr>

  <!-- =========================== -->
  <!-- Part 2.1 -->
  <!-- =========================== -->
  <h2 id="part2-1">Part 2.1: Create Rays from Cameras</h2>

  <h3>Camera to World Coordinate Conversion</h3>
  <p>
    The transformation between world coordinates <code>ùêó_w</code> and camera coordinates <code>ùêó_c</code>
    can be defined using a rotation matrix <code>R</code> and a translation vector <code>t</code>:
  </p>
  <p style="text-align: center;">
    $$\mathbf{X}_c = R \mathbf{X}_w + t$$
  </p>
  <p>
    The combined matrix is called the world-to-camera (w2c) transformation or extrinsic matrix. Its inverse
    gives the camera-to-world (c2w) transformation. In this session, we implement a function
    <code>x_w = transform(c2w, x_c)</code> that converts points from camera space to world space. We verify
    correctness using
  </p>
  <p style="text-align: center;">
    $$\mathbf{x} = \text{transform}(\text{c2w}^{-1}, \text{transform}(\text{c2w}, \mathbf{x}))$$
  </p>

  <h3>Pixel to Camera Coordinate Conversion</h3>
  <p>
    Consider a pinhole camera with intrinsic matrix
  </p>
  <p style="text-align: center;">
    $$K = \begin{bmatrix} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end{bmatrix}$$
  </p>
  <p>
    A 3D point in camera space <code>ùêó_c = [X_c, Y_c, Z_c]^T</code> can be projected to pixel coordinates <code>uv</code>:
  </p>
  <p style="text-align: center;">
    $$uv = K \frac{\mathbf{X}_c}{Z_c}$$
  </p>
  <p>
    We implement the inverse function <code>pixel_to_camera(K, uv, s)</code> to map a pixel coordinate back
    to a point in camera space. Both functions support batched coordinates for efficiency.
  </p>

  <h3>Pixel to Ray</h3>
  <p>
    Each pixel defines a ray with origin <code>r_o</code> and direction <code>r_d</code>. The camera origin
    in world space is given by the translation component of the <code>c2w</code> matrix. To compute the
    ray direction, we choose a point along the ray with depth 1 in camera coordinates and transform it to
    world coordinates. Then we normalize the vector to get <code>r_d</code>.
  </p>
  <p style="text-align: center; font-weight: bold; font-size: 18px;">
    $$\mathbf{x}(t) = \mathbf{r}_o + t \mathbf{r}_d$$
  </p>

  <hr>

  <!-- =========================== -->
  <!-- Part 2.2 -->
  <!-- =========================== -->
  <h2 id="part2-2">Part 2.2: Sampling</h2>

  <p>
    Once rays are defined for all pixels in all views, we sample a subset of rays for training. Two options
    exist: (1) sample a few images and then sample rays from each, or (2) flatten all pixels across all images
    and sample globally. Each ray consists of an origin and a direction, along with its corresponding
    RGB color from the image.
  </p>

  <h3>Sampling Points along Rays</h3>
  <p>
    To train NeRF, we discretize each ray into a set of 3D sample points. Uniformly sample along the ray between
    near and far bounds, for example, <code>near=2.0</code> and <code>far=6.0</code>. To reduce overfitting, we
    add a small random perturbation to each sample during training. The 3D coordinates for each point along the
    ray are then given by:
  </p>
  <p style="text-align: center; font-weight: bold; font-size: 18px;">
    $$\mathbf{x}(t) = \mathbf{r}_o + t \mathbf{r}_d$$
  </p>
  <p>
    Here, <code>n_samples</code> can be set to 32 or 64. These 3D points along the rays, together with their
    viewing directions, are used as input to the NeRF MLP for predicting RGB and density values.
  </p>
</div>

   <!-- =========================== -->
<!-- Part 2.3 -->
<!-- =========================== -->
<div class="section" id="part2-3">
  <h2>Part 2.3: Putting the Dataloading All Together</h2>

  <p>
    In this part, I implemented a dataloader that randomly samples pixels from all multi-view images.
    For each sampled pixel, I converted its 2D coordinates into a ray with origin and direction in world
    coordinates, and returned the corresponding RGB color. To verify that everything works correctly, I
    visualized the rays and sample points in 3D. I also tried sampling rays from a single camera to
    ensure that they lie inside the camera frustum and to catch any possible errors.
  </p>

  <!-- Example images -->
  <div style="display: flex; justify-content: center; gap: 30px; margin-top: 30px;">
    <!-- Image 1: Random 100 rays -->
    <div style="flex: 0 0 400px; text-align: center;">
      <img src="random_100_rays.png" alt="Random 100 Rays" width="400">
      <p>Random 100 Rays</p>
    </div>

    <!-- Image 2: 100 rays from one camera -->
    <div style="flex: 0 0 400px; text-align: center;">
      <img src="single_camera_100_rays.png" alt="100 Rays from One Camera" width="400">
      <p>100 Rays from One Camera</p>
    </div>
  </div>

</div>
<!-- =========================== -->
<!-- Part 2.4 -->
<!-- =========================== -->
<div class="section" id="part2_4">
  <h2>Part 2.4: Neural Radiance Field</h2>

  <p>
    After sampling 3D points along rays, the next step is to build the <b>Neural Radiance Field (NeRF)</b>.
    The network predicts both the <b>density</b> and <b>color</b> for each 3D point in world space.
    Compared with the 2D neural field in Part 1, there are several key changes:
  </p>

  <ul>
    <li>
      <b>Input:</b> 3D world coordinates of each sample point, along with the 3D ray direction.
      The view direction conditions the predicted color because the radiance depends on the viewing angle.
    </li>
    <li>
      <b>Output:</b> Density (œÉ) and color (RGB). Density is constrained to be non-negative using <code>ReLU</code>,
      while color is constrained to <code>[0,1]</code> with <code>Sigmoid</code>.
    </li>
    <li>
      <b>Positional Encoding:</b> Apply sinusoidal PE to both coordinates and ray directions.
      Coordinate PE can use a high frequency (e.g., L=10) while direction PE can use a lower frequency (e.g., L=4).
    </li>
    <li>
      <b>Deeper MLP:</b> Since modeling a 3D scene is more challenging, a deeper and wider MLP is used to improve capacity.
    </li>
    <li>
      <b>Input Injection:</b> After positional encoding, the input is also concatenated to the middle of the MLP.
      This helps the network retain spatial information and improves training stability.
    </li>
  </ul>

  <p>
    The network structure is illustrated below. You can start from this design and adjust depth, width, and
    PE frequencies based on your data and computational resources.
  </p>

  <div style="text-align: center; margin-top: 20px;">
    <img src="intronerf.png" width="700">
  </div>
</div>

<!-- =========================== -->
<!-- Part 2.5 -->
<!-- =========================== -->
<div class="section" id="part2-5">
  <h2>Part 2.5: Volume Rendering</h2>

  <p>
    In this part, I implemented the <b>volume rendering</b> equation for NeRF. The core idea
    is that along each ray, the color is computed by accumulating the contributions of all
    samples weighted by their density and transmittance. In continuous form:
  </p>

  <p style="text-align: center;">
    \( C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \, \sigma(\mathbf{r}(t)) \, \mathbf{c}(\mathbf{r}(t), \mathbf{d}) \, dt, \quad
    \text{where } T(t) = \exp\Big(-\int_{t_n}^t \sigma(\mathbf{r}(s)) ds\Big) \)
  </p>

  <p>
    For computation, I used the discrete approximation:
  </p>

  <p style="text-align: center;">
    \( \hat{C}(\mathbf{r}) = \sum_{i=1}^N T_i \left(1 - \exp(-\sigma_i \delta_i)\right) \mathbf{c}_i, \quad
    T_i = \exp\Big(-\sum_{j=1}^{i-1} \sigma_j \delta_j\Big) \)
  </p>

  <p>
    In practice, I implemented this in <b>PyTorch</b> to allow backpropagation. For each ray,
    I first sampled points along the ray, predicted their density (\(\sigma_i\)) and color (\(\mathbf{c}_i\))
    using the NeRF MLP, then applied the discrete volume rendering formula to get the final
    RGB value for that ray.
  </p>

  <p>
    I also visualized up to 100 rays and the corresponding sample points at a single training
    step to ensure the rays correctly lie within the camera frustum.
  </p>

  <!-- Process images row -->
  <div style="
      display: flex;
      justify-content: center;
      gap: 15px;
      margin-top: 20px;
  ">
    <div style="text-align: center;">
      <img src="lego100.png" width="160">
      <p style="margin-top: 6px;">Iteration 100</p>
    </div>

    <div style="text-align: center;">
      <img src="lego500.png" width="160">
      <p style="margin-top: 6px;">Iteration 500</p>
    </div>

    <div style="text-align: center;">
      <img src="lego1000.png" width="160">
      <p style="margin-top: 6px;">Iteration 1000</p>
    </div>

    <div style="text-align: center;">
      <img src="lego1500.png" width="160">
      <p style="margin-top: 6px;">Iteration 1500</p>
    </div>

    <div style="text-align: center;">
      <img src="lego2000.png" width="160">
      <p style="margin-top: 6px;">Iteration 2000</p>
    </div>
  </div>

  <!-- PSNR and Loss curves -->
  <p style="margin-top: 30px; text-align: center;">
    Below (left) shows the Validation PSNR Curve, which reaches 25 after 2000 iterations. 
    The training loss (right) curve goes down rapidly at the beginning and becomes stable in the later stage of training.
  </p>

  <div style="display: flex; justify-content: center; gap: 30px; margin-top: 10px;">
    <div style="text-align: center;">
      <img src="legoPSNR.png" width="400">
      <p style="margin-top: 6px;">Validation PSNR Curve</p>
    </div>

    <div style="text-align: center;">
      <img src="legoloss.png" width="400">
      <p style="margin-top: 6px;">Training Loss Curve</p>
    </div>
  </div>

  <!-- Spherical renderings GIF -->
  <p style="margin-top: 30px;">
    Here is the spherical renderings video of the Lego:
  </p>

  <div style="text-align: center; margin-top: 10px;">
    <img src="goodrendered.gif" width="400" loop="infinite">
  </div>

</div>

   
    
  </div>
</body>
</html>
