<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Project1: Images of the Russian Empire ‚Äî Part 0</title>
  <style>
    body {
      background-color: #D8C3DD; /* Á¥´Ëâ≤ËÉåÊôØÔºà‰øùÊåÅÂéüÊù•È¢úËâ≤Ôºâ */
      font-family: Arial, sans-serif;
      color: white;
      margin: 0;
      padding: 0;
    }
    /* ÂØºËà™Ê†èÔºàÈ¢úËâ≤‰∏éÊ†∑Âºè‰øùÊåÅ‰∏çÂèòÔºâ */
    nav {
      background: rgba(0, 0, 0, 0.3);
      padding: 10px 20px;
      position: sticky;
      top: 0;
      z-index: 1000;
    }
    nav a {
      color: #ffddff;
      text-decoration: none;
      margin: 0 15px;
      font-weight: bold;
    }
    nav a:hover {
      text-decoration: underline;
    }
    .container {
      width: 80%;
      margin: auto;
      padding: 20px;
    }
    h1 {
      text-align: center;
      margin-top: 20px;
    }
    .section {
      background: rgba(0,0,0,0.2);
      padding: 20px;
      margin: 40px 0;
      border-radius: 12px;
    }
    .section h2 {
      color: #ffddff;
      text-align: center;
      margin-top: 0;
    }
    p {
      text-align: left;
      max-width: 1000px;
      margin: auto;
      margin-bottom: 20px;
      font-size: 18px;
      color: white;
    }
    hr {
      border: none;
      border-top: 1px solid rgba(255,255,255,0.12);
      margin: 30px 0;
    }

    /* ÂõæÁâáÊéíÁâàÂæÆË∞ÉÔºàÊõ¥Â§ß„ÄÅÊõ¥Â±Ö‰∏≠„ÄÅÈó¥Ë∑ùÊõ¥Â§ßÔºâ */
    .two-images {
      display: flex;
      justify-content: center;
      gap: 48px; /* ‰∏≠Èó¥Á©∫‰∏ÄÁÇπ */
      margin-top: 30px;
      flex-wrap: wrap;
    }
    .img-block {
      flex: 0 0 320px; /* Âü∫ÂáÜÊõ¥Â§ß */
      text-align: center;
    }
    .img-block img {
      width: 400px; /* ÊîæÂ§ßÂõæÁâá */
      border-radius: 8px;
      border: 1px solid rgba(255,255,255,0.08);
    }
    .img-block p {
      margin-top: 8px;
      font-size: 16px;
      color: white;
      text-align: center;
    }

    /* ‰øùËØÅÂ∞èÂ±èÂπïÈÄÇÈÖç */
    @media (max-width: 760px) {
      .container { width: 92%; padding: 12px; }
      .img-block { flex: 0 0 45%; }
      .img-block img { width: 100%; }
    }
  </style>
  <!-- MathJax kept if needed later -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <nav>
    <a href="#part0">Part0</a>
    <a href="#part1">Part1</a>
    <a href="#part2">Part2</a>
    <a href="#part3">Part3</a>
  </nav>

  <div class="container">
    <h1>Project1: Images of the Russian Empire ‚Äî Calibration & 3D Scan (Part 0)</h1>

    <!-- =========================== -->
    <!-- Part 0.0 -->
    <!-- =========================== -->
    <div class="section" id="part0">
      <h2>Part 0.0: Calibrating My Camera and Capturing a 3D Scan</h2>

      <h2 id="part0-1">Part 0.1: Calibrating My Camera</h2>

      <p>
        Following the assignment instructions, I first captured a set of calibration images
        containing multiple ArUco markers. I printed the calibration tag sheet and took
        around 40 images using my phone camera, keeping the zoom fixed while varying the
        viewing angle and distance. Phone cameras work well for this task since their images
        are already mostly undistorted.
      </p>

      <p>
        In my calibration script, I looped through all captured images and used OpenCV‚Äôs
        ArUco detector to locate the markers. For each detected tag, I extracted the 2D
        corner coordinates. Since the real-world physical size of each tag is known, I
        defined the 3D corner points accordingly (e.g., a 2cm √ó 2cm square mapped to
        four 3D points lying on the z=0 plane).
      </p>

      <p>
        After collecting all 2D‚Äì3D correspondences from all images, I used
        <code>cv2.calibrateCamera()</code> to estimate my camera intrinsics and distortion
        coefficients. These calibration parameters will later be used to compute object
        pose for the 3D scan.
      </p>

      <hr>

      <h2 id="part0-2">Part 0.2: Capturing the 3D Scan</h2>

      <p>
        With camera intrinsics estimated, I proceeded to capture image data for the 3D scan.
        I photographed the object from many viewpoints with substantial overlap (roughly 60‚Äì80%),
        maintaining consistent exposure and diffuse lighting to avoid harsh shadows. When possible,
        I placed the object on a simple turntable and rotated it incrementally; otherwise I walked
        the camera around the object while keeping the object centered in the frame.
      </p>

      <p>
        For robust reconstruction I captured images at multiple elevation angles (top, mid, low)
        and ensured good coverage of concave regions. I checked for motion blur and removed any
        blurry frames. All photos were saved in a single folder and named sequentially to preserve
        ordering.
      </p>

      <p>
        In post-processing I undistorted each image using the previously computed distortion
        coefficients, so the reconstruction algorithm receives geometrically-correct images.
        I then fed the undistorted image set and the camera intrinsics into a photogrammetry
        pipeline (e.g., COLMAP / OpenMVG + OpenMVS) to compute camera poses, sparse and dense
        point clouds, and finally a textured mesh (PLY/OBJ). The resulting 3D model can be
        aligned with the calibration coordinate frame so that measured 3D points correspond to
        the real-world scale used during calibration.
      </p>

      <p>
        This calibrated scan workflow ensures that the 3D geometry and the camera parameters are
        mutually consistent, which is important for later steps such as precise pose estimation,
        metric measurements, or combining multi-modal data.
      </p>
    </div>

    <!-- =========================== -->
    <!-- Part 0.3 -->
    <!-- =========================== -->
    <div class="section" id="part0-3-section">
      <h2 id="part0-3">Part 0.3: Estimating Camera Pose</h2>

      <p>
        With the camera intrinsics calibrated in Part 0.1, I next estimated the camera pose
        (its 3D position and orientation) for every image in my object scan. This is a classic
        Perspective-n-Point (PnP) problem: given several known 3D points and their 2D projections,
        we solve for the camera's extrinsic parameters.
      </p>

      <p>
        In my case, each scan image contains a single ArUco tag placed next to the object.
        For every image, I used OpenCV‚Äôs ArUco detector to obtain the 2D pixel coordinates of the
        tag‚Äôs four corners. Since the physical size of my printed tag is known, I defined its
        corresponding 3D corner coordinates in meters on a flat z=0 plane.
      </p>

      <p>
        Using these 2D‚Äì3D correspondences, I called <code>cv2.solvePnP()</code> to compute the tag-to-camera
        rotation and translation. I then converted the returned axis-angle vector to a rotation matrix 
        via <code>cv2.Rodrigues()</code>. Since OpenCV gives the world-to-camera transform, I inverted it to
        obtain the camera-to-world (c2w) pose matrix required for visualization and NeRF-style pipelines.
      </p>

      <!-- Example images for Part 0.3 (two images, centered, larger) -->
      <div class="two-images" role="group" aria-label="Pose examples">
        <div class="img-block">
          <img src="pose1.jpg" alt="Pose Visualization 1">
          <p>Estimated Pose (Example 1)</p>
        </div>

        <div class="img-block">
          <img src="pose2.jpg" alt="Pose Visualization 2">
          <p>Estimated Pose (Example 2)</p>
        </div>
      </div>
    </div>

    <hr>

    <!-- =========================== -->
    <!-- Part 0.4 -->
    <!-- =========================== -->
    <div class="section" id="part0-4-section">
      <h2 id="part0-4">Part 0.4: Undistorting Images and Building the Dataset</h2>

      <p>
        After estimating the pose for each image, I prepared the final dataset by undistorting all
        images using the calibration parameters. Since NeRF assumes a perfect pinhole camera model,
        removing radial and tangential distortion is important for geometric consistency.
      </p>

      <p>
        I used <code>cv2.undistort()</code> together with the calibrated intrinsics and distortion 
        coefficients to obtain clean, geometrically-correct images. Some images exhibit black borders 
        after undistortion; for these cases, I optionally used 
        <code>cv2.getOptimalNewCameraMatrix()</code> to compute a cropped intrinsics matrix and a valid ROI,
        ensuring that the principal point is updated whenever cropping occurs.
      </p>

      <p>
        Finally, I organized all undistorted images, camera intrinsics, and camera-to-world pose matrices
        into a structured dataset (following the JSON / transforms schema used in NeRF pipelines).  
        This dataset will serve as the input for dense reconstruction and rendering in later parts
        of the project.
      </p>
    </div>

<!-- =========================== -->
<!-- Part 1 -->
<!-- =========================== -->
<div class="section" id="part1">
  <h2>Part 1: Fit a Neural Field to a 2D Image</h2>

  <p>
    Before moving to a full 3D Neural Radiance Field (NeRF), this section uses a
    <b>2D Neural Field</b> to build intuition. In the 2D case there is no notion of radiance or
    volume density, so NeRF simplifies into a function that maps a 2D pixel coordinate
    <i>(x, y)</i> to an RGB color <i>(r, g, b)</i>. The goal is to learn this function using a
    small MLP with positional encoding.
  </p>

  <p>
    I followed the assignment instructions and implemented the following components:
  </p>

  <p>
    <b>1. Positional Encoding (PE)</b><br>
    I applied sinusoidal positional encoding to each 2D coordinate, following the formulation
    from the NeRF paper. With a maximum frequency level of <i>L = 10</i>, the original
    2D input was expanded to a 42-dimensional vector. High-frequency PE allows the network
    to reproduce sharp edges and fine structures in the target image.
  </p>

  <p>
    <b>2. Multilayer Perceptron (MLP)</b><br>
    I implemented an MLP that takes the encoded coordinates as input and outputs a
    3-dimensional RGB value. The network ends with a <code>Sigmoid</code> layer to ensure that the
    predicted colors fall within <i>[0, 1]</i>. The input image was also normalized to this range
    for supervision.
  </p>

  <p>
    <b>3. Dataloader with Random Pixel Sampling</b><br>
    To avoid GPU memory issues when training on high-resolution images, I wrote a dataloader
    that randomly samples <i>10,000</i> pixels every iteration. The dataloader returns normalized
    pixel coordinates <i>(x / W, y / H)</i> and their corresponding RGB values.
  </p>

  <p>
    <b>4. Loss Function, Optimizer, and Metrics</b><br>
    The network was trained using <code>MSELoss</code> between predicted and ground-truth colors.
    I used the Adam optimizer with a learning rate of <code>1e-2</code> and trained for
    1000‚Äì3000 iterations. Reconstruction quality was evaluated using the Peak
    Signal-to-Noise Ratio (PSNR), computed from the MSE.
  </p>

  <p>
    <b>5. Hyperparameter Tuning</b><br>
    I experimented with different MLP widths and different PE frequency levels to see
    how they affect convergence and reconstruction sharpness. Below are the results after
    200, 800, 1400, and 2000 training iterations.
  </p>

  <hr>

  <!-- Image row (4 images) -->
  <div style="
      display: flex;
      justify-content: center;
      gap: 25px;
      margin-top: 20px;
  ">
    <div style="text-align: center;">
      <img src="fox200.png" width="300">
      <p style="margin-top: 8px;">Iteration 200</p>
    </div>

    <div style="text-align: center;">
      <img src="fox800.png" width="300">
      <p style="margin-top: 8px;">Iteration 800</p>
    </div>

    <div style="text-align: center;">
      <img src="fox1400.png" width="300">
      <p style="margin-top: 8px;">Iteration 1400</p>
    </div>

    <div style="text-align: center;">
      <img src="fox2000.png" width="300">
      <p style="margin-top: 8px;">Iteration 2000</p>
    </div>
  </div>

</div>

<!-- =========================== -->
<!-- Part 2 -->
<!-- =========================== -->
<div class="section" id="part2">
  <h2>Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>

  <p>
    In this part, we extend the neural field concept from Part 1 to a full 3D Neural Radiance Field (NeRF).
    Instead of a single 2D image, we now have multiple images of the object from different viewpoints, along
    with camera intrinsics and extrinsics. The goal is to represent the 3D scene and learn a continuous
    function that maps 3D points and viewing directions to RGB colors and densities.
  </p>

  <hr>

  <!-- =========================== -->
  <!-- Part 2.1 -->
  <!-- =========================== -->
  <h2 id="part2-1">Part 2.1: Create Rays from Cameras</h2>

  <h3>Camera to World Coordinate Conversion</h3>
  <p>
    The transformation between world coordinates <code>ùêó_w</code> and camera coordinates <code>ùêó_c</code>
    can be defined using a rotation matrix <code>R</code> and a translation vector <code>t</code>:
  </p>
  <p style="text-align: center;">
    $$\mathbf{X}_c = R \mathbf{X}_w + t$$
  </p>
  <p>
    The combined matrix is called the world-to-camera (w2c) transformation or extrinsic matrix. Its inverse
    gives the camera-to-world (c2w) transformation. In this session, we implement a function
    <code>x_w = transform(c2w, x_c)</code> that converts points from camera space to world space. We verify
    correctness using
  </p>
  <p style="text-align: center;">
    $$\mathbf{x} = \text{transform}(\text{c2w}^{-1}, \text{transform}(\text{c2w}, \mathbf{x}))$$
  </p>

  <h3>Pixel to Camera Coordinate Conversion</h3>
  <p>
    Consider a pinhole camera with intrinsic matrix
  </p>
  <p style="text-align: center;">
    $$K = \begin{bmatrix} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end{bmatrix}$$
  </p>
  <p>
    A 3D point in camera space <code>ùêó_c = [X_c, Y_c, Z_c]^T</code> can be projected to pixel coordinates <code>uv</code>:
  </p>
  <p style="text-align: center;">
    $$uv = K \frac{\mathbf{X}_c}{Z_c}$$
  </p>
  <p>
    We implement the inverse function <code>pixel_to_camera(K, uv, s)</code> to map a pixel coordinate back
    to a point in camera space. Both functions support batched coordinates for efficiency.
  </p>

  <h3>Pixel to Ray</h3>
  <p>
    Each pixel defines a ray with origin <code>r_o</code> and direction <code>r_d</code>. The camera origin
    in world space is given by the translation component of the <code>c2w</code> matrix. To compute the
    ray direction, we choose a point along the ray with depth 1 in camera coordinates and transform it to
    world coordinates. Then we normalize the vector to get <code>r_d</code>.
  </p>
  <p style="text-align: center; font-weight: bold; font-size: 18px;">
    $$\mathbf{x}(t) = \mathbf{r}_o + t \mathbf{r}_d$$
  </p>

  <hr>

  <!-- =========================== -->
  <!-- Part 2.2 -->
  <!-- =========================== -->
  <h2 id="part2-2">Part 2.2: Sampling</h2>

  <p>
    Once rays are defined for all pixels in all views, we sample a subset of rays for training. Two options
    exist: (1) sample a few images and then sample rays from each, or (2) flatten all pixels across all images
    and sample globally. Each ray consists of an origin and a direction, along with its corresponding
    RGB color from the image.
  </p>

  <h3>Sampling Points along Rays</h3>
  <p>
    To train NeRF, we discretize each ray into a set of 3D sample points. Uniformly sample along the ray between
    near and far bounds, for example, <code>near=2.0</code> and <code>far=6.0</code>. To reduce overfitting, we
    add a small random perturbation to each sample during training. The 3D coordinates for each point along the
    ray are then given by:
  </p>
  <p style="text-align: center; font-weight: bold; font-size: 18px;">
    $$\mathbf{x}(t) = \mathbf{r}_o + t \mathbf{r}_d$$
  </p>
  <p>
    Here, <code>n_samples</code> can be set to 32 or 64. These 3D points along the rays, together with their
    viewing directions, are used as input to the NeRF MLP for predicting RGB and density values.
  </p>
</div>
    
    
  </div>
</body>
</html>
