<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Project1: Images of the Russian Empire — Part 0</title>
  <style>
    body {
      background-color: #D8C3DD; /* 紫色背景（保持原来颜色） */
      font-family: Arial, sans-serif;
      color: white;
      margin: 0;
      padding: 0;
    }
    /* 导航栏（颜色与样式保持不变） */
    nav {
      background: rgba(0, 0, 0, 0.3);
      padding: 10px 20px;
      position: sticky;
      top: 0;
      z-index: 1000;
    }
    nav a {
      color: #ffddff;
      text-decoration: none;
      margin: 0 15px;
      font-weight: bold;
    }
    nav a:hover {
      text-decoration: underline;
    }
    .container {
      width: 80%;
      margin: auto;
      padding: 20px;
    }
    h1 {
      text-align: center;
      margin-top: 20px;
    }
    .section {
      background: rgba(0,0,0,0.2);
      padding: 20px;
      margin: 40px 0;
      border-radius: 12px;
    }
    .section h2 {
      color: #ffddff;
      text-align: center;
      margin-top: 0;
    }
    p {
      text-align: left;
      max-width: 1000px;
      margin: auto;
      margin-bottom: 20px;
      font-size: 18px;
      color: white;
    }
    hr {
      border: none;
      border-top: 1px solid rgba(255,255,255,0.12);
      margin: 30px 0;
    }

    /* 图片排版微调（更大、更居中、间距更大） */
    .two-images {
      display: flex;
      justify-content: center;
      gap: 48px; /* 中间空一点 */
      margin-top: 30px;
      flex-wrap: wrap;
    }
    .img-block {
      flex: 0 0 320px; /* 基准更大 */
      text-align: center;
    }
    .img-block img {
      width: 450px; /* 放大图片 */
      border-radius: 8px;
      border: 1px solid rgba(255,255,255,0.08);
    }
    .img-block p {
      margin-top: 8px;
      font-size: 16px;
      color: white;
      text-align: center;
    }

    /* 保证小屏幕适配 */
    @media (max-width: 760px) {
      .container { width: 92%; padding: 12px; }
      .img-block { flex: 0 0 45%; }
      .img-block img { width: 100%; }
    }
  </style>
  <!-- MathJax kept if needed later -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <nav>
    <a href="#part0">Part0</a>
    <a href="#part1">Part1</a>
    <a href="#part2">Part2</a>
    <a href="#part3">Part3</a>
  </nav>

  <div class="container">
    <h1>Project1: Images of the Russian Empire — Calibration & 3D Scan (Part 0)</h1>

    <!-- =========================== -->
    <!-- Part 0.0 -->
    <!-- =========================== -->
    <div class="section" id="part0">
      <h2>Part 0.0: Calibrating My Camera and Capturing a 3D Scan</h2>

      <h2 id="part0-1">Part 0.1: Calibrating My Camera</h2>

      <p>
        Following the assignment instructions, I first captured a set of calibration images
        containing multiple ArUco markers. I printed the calibration tag sheet and took
        around 40 images using my phone camera, keeping the zoom fixed while varying the
        viewing angle and distance. Phone cameras work well for this task since their images
        are already mostly undistorted.
      </p>

      <p>
        In my calibration script, I looped through all captured images and used OpenCV’s
        ArUco detector to locate the markers. For each detected tag, I extracted the 2D
        corner coordinates. Since the real-world physical size of each tag is known, I
        defined the 3D corner points accordingly (e.g., a 2cm × 2cm square mapped to
        four 3D points lying on the z=0 plane).
      </p>

      <p>
        After collecting all 2D–3D correspondences from all images, I used
        <code>cv2.calibrateCamera()</code> to estimate my camera intrinsics and distortion
        coefficients. These calibration parameters will later be used to compute object
        pose for the 3D scan.
      </p>

      <hr>

      <h2 id="part0-2">Part 0.2: Capturing the 3D Scan</h2>

      <p>
        With camera intrinsics estimated, I proceeded to capture image data for the 3D scan.
        I photographed the object from many viewpoints with substantial overlap (roughly 60–80%),
        maintaining consistent exposure and diffuse lighting to avoid harsh shadows. When possible,
        I placed the object on a simple turntable and rotated it incrementally; otherwise I walked
        the camera around the object while keeping the object centered in the frame.
      </p>

      <p>
        For robust reconstruction I captured images at multiple elevation angles (top, mid, low)
        and ensured good coverage of concave regions. I checked for motion blur and removed any
        blurry frames. All photos were saved in a single folder and named sequentially to preserve
        ordering.
      </p>

      <p>
        In post-processing I undistorted each image using the previously computed distortion
        coefficients, so the reconstruction algorithm receives geometrically-correct images.
        I then fed the undistorted image set and the camera intrinsics into a photogrammetry
        pipeline (e.g., COLMAP / OpenMVG + OpenMVS) to compute camera poses, sparse and dense
        point clouds, and finally a textured mesh (PLY/OBJ). The resulting 3D model can be
        aligned with the calibration coordinate frame so that measured 3D points correspond to
        the real-world scale used during calibration.
      </p>

      <p>
        This calibrated scan workflow ensures that the 3D geometry and the camera parameters are
        mutually consistent, which is important for later steps such as precise pose estimation,
        metric measurements, or combining multi-modal data.
      </p>
    </div>

    <!-- =========================== -->
    <!-- Part 0.3 -->
    <!-- =========================== -->
    <div class="section" id="part0-3-section">
      <h2 id="part0-3">Part 0.3: Estimating Camera Pose</h2>

      <p>
        With the camera intrinsics calibrated in Part 0.1, I next estimated the camera pose
        (its 3D position and orientation) for every image in my object scan. This is a classic
        Perspective-n-Point (PnP) problem: given several known 3D points and their 2D projections,
        we solve for the camera's extrinsic parameters.
      </p>

      <p>
        In my case, each scan image contains a single ArUco tag placed next to the object.
        For every image, I used OpenCV’s ArUco detector to obtain the 2D pixel coordinates of the
        tag’s four corners. Since the physical size of my printed tag is known, I defined its
        corresponding 3D corner coordinates in meters on a flat z=0 plane.
      </p>

      <p>
        Using these 2D–3D correspondences, I called <code>cv2.solvePnP()</code> to compute the tag-to-camera
        rotation and translation. I then converted the returned axis-angle vector to a rotation matrix 
        via <code>cv2.Rodrigues()</code>. Since OpenCV gives the world-to-camera transform, I inverted it to
        obtain the camera-to-world (c2w) pose matrix required for visualization and NeRF-style pipelines.
      </p>

      <!-- Example images for Part 0.3 (two images, centered, larger) -->
      <div class="two-images" role="group" aria-label="Pose examples">
        <div class="img-block">
          <img src="pose1.jpg" alt="Pose Visualization 1">
          <p>Estimated Pose (Example 1)</p>
        </div>

        <div class="img-block">
          <img src="pose2.jpg" alt="Pose Visualization 2">
          <p>Estimated Pose (Example 2)</p>
        </div>
      </div>
    </div>

    <hr>

    <!-- =========================== -->
    <!-- Part 0.4 -->
    <!-- =========================== -->
    <div class="section" id="part0-4-section">
      <h2 id="part0-4">Part 0.4: Undistorting Images and Building the Dataset</h2>

      <p>
        After estimating the pose for each image, I prepared the final dataset by undistorting all
        images using the calibration parameters. Since NeRF assumes a perfect pinhole camera model,
        removing radial and tangential distortion is important for geometric consistency.
      </p>

      <p>
        I used <code>cv2.undistort()</code> together with the calibrated intrinsics and distortion 
        coefficients to obtain clean, geometrically-correct images. Some images exhibit black borders 
        after undistortion; for these cases, I optionally used 
        <code>cv2.getOptimalNewCameraMatrix()</code> to compute a cropped intrinsics matrix and a valid ROI,
        ensuring that the principal point is updated whenever cropping occurs.
      </p>

      <p>
        Finally, I organized all undistorted images, camera intrinsics, and camera-to-world pose matrices
        into a structured dataset (following the JSON / transforms schema used in NeRF pipelines).  
        This dataset will serve as the input for dense reconstruction and rendering in later parts
        of the project.
      </p>
    </div>

  </div>
</body>
</html>
